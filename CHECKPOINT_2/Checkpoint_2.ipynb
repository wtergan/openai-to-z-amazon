{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Checkpoint 2: Anomaly Detection with GEDI, PRODES, SRTM, LiDAR & Sentinel-2**\n",
    "\n",
    "This notebook implements a full pipeline for detecting anomalous archaeological features in the Amazon. It integrates multiple remote sensing datasets, performs advanced feature engineering, scores potential anomalies, and uses a Large Language Model (LLM) for a final contextual assessment.\n",
    "\n",
    "The process for each is:\n",
    "\n",
    "1. Setup: Install libraries and configure API keys.\n",
    "2. Configuration: Define the region of interest (bounding box), year, and other pipeline parameters.\n",
    "3. Data Fetching:\n",
    "    - Fetch GEDI, PRODES, and SRTM data for fine-grained, per-cell analysis.\n",
    "    - Fetch regional LiDAR and Sentinel-2 imagery for broader context.\n",
    "4. Feature Extraction & Engineering:\n",
    "    - Extract visual features and statistics from LiDAR and Sentinel-2 data.\n",
    "    - Process GEDI, PRODES, and SRTM data into a rich feature set for each H3 hexagonal cell.\n",
    "5. Anomaly Detection:\n",
    "    - Score each cell based on its features to identify potential anomalies.\n",
    "    - Rank the cells to find the top N most anomalous candidates.\n",
    "6. LLM-Powered Analysis:\n",
    "    - Regional Assessment: The LLM first analyzes the regional LiDAR and Sentinel-2 imagery to provide a high-level summary of the area's characteristics.\n",
    "    - Per-Cell Assessment: The LLM then assesses each of the top N anomalous cells, using the regional assessment for context, to determine its archaeological potential.\n",
    "7. Results: Display the regional summary and the detailed assessment for each top-ranked cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and API Key Configuration\n",
    "\n",
    "This section installs all required Python packages. It also sets up authentication for Google Earth Engine (GEE) and the LLM providers (OpenAI/OpenRouter), as well as configuration for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# INSTALLING DEPENDENCIES\n",
    "# ===============================================================================\n",
    "!pip install -q numpy pandas scipy geopandas shapely h3 owslib scikit-learn earthengine-api requests urllib3 pyarrow python-dotenv laspy rasterio matplotlib zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# IMPORTING LIBRARIES\n",
    "# ===============================================================================\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import hashlib\n",
    "import tempfile\n",
    "import h3\n",
    "import zipfile\n",
    "import base64\n",
    "import urllib.request\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "from urllib.parse import urljoin\n",
    "from functools import lru_cache\n",
    "from datetime import datetime\n",
    "from getpass import getpass\n",
    "\n",
    "import ee\n",
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import box, Point, Polygon\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from owslib.wfs import WebFeatureService\n",
    "from IPython.display import display, Image as IPImage, JSON, Markdown\n",
    "\n",
    "import numpy as np\n",
    "import laspy\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LightSource\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "from openai import OpenAI, OpenAIError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# API KEY CONFIGURATION\n",
    "# ===============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Loading the environment variables from .env file:\n",
    "try:\n",
    "    load_dotenv()\n",
    "    logger.info(\".env file loaded.\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not load .env file: {e}. Please enter keys manually.\")\n",
    "\n",
    "# Setting up API keys and GEE Project ID:\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OT_API_KEY = os.getenv(\"OT_API_KEY\")\n",
    "GEE_PROJECT_ID = os.getenv(\"GEE_PROJECT_ID\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    OPENAI_API_KEY = getpass(\"Enter your OpenAI API Key: \")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    OPENROUTER_API_KEY = getpass(\"Enter your OpenRouter API Key: \")\n",
    "if not OT_API_KEY:\n",
    "    OT_API_KEY = getpass(\"Enter your OpenTopography API Key: \")\n",
    "if not GEE_PROJECT_ID:\n",
    "    GEE_PROJECT_ID = getpass(\"Enter your GEE Project ID: \")\n",
    "\n",
    "# Initializing Google Earth Engine:\n",
    "gee_initialized_successfully = False\n",
    "def initialize_gee():\n",
    "    \"\"\"Initialize Google Earth Engine with proper authentication handling.\"\"\"\n",
    "    global gee_initialized_successfully\n",
    "    if gee_initialized_successfully:\n",
    "        return True\n",
    "    try:\n",
    "        logger.info(\"Attempting GEE initialization...\")\n",
    "        if GEE_PROJECT_ID:\n",
    "            ee.Initialize(project=GEE_PROJECT_ID, opt_url='https://earthengine.googleapis.com')\n",
    "        else:\n",
    "            ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "        logger.info(\"GEE initialized successfully.\")\n",
    "        gee_initialized_successfully = True\n",
    "    except ee.EEException as e_init:\n",
    "        logger.warning(f\"GEE auto-initialization failed: {e_init}. Attempting authentication flow.\")\n",
    "        try:\n",
    "            ee.Authenticate()\n",
    "            if GEE_PROJECT_ID:\n",
    "                ee.Initialize(project=GEE_PROJECT_ID, opt_url='https://earthengine.googleapis.com')\n",
    "            else:\n",
    "                ee.Initialize(opt_url='https://earthengine.googleapis.com')\n",
    "            logger.info(\"GEE authenticated and initialized successfully.\")\n",
    "            gee_initialized_successfully = True\n",
    "        except Exception as e_auth:\n",
    "            logger.error(f\"CRITICAL: GEE authentication and initialization failed: {e_auth}\")\n",
    "            gee_initialized_successfully = False\n",
    "    return gee_initialized_successfully\n",
    "\n",
    "# Initializing GEE right away:\n",
    "initialize_gee()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Configuration:\n",
    "\n",
    "Setting up pertinent parameters for the assessment pipeline:\n",
    "- BBOX: The bounding box [min_lon, min_lat, max_lon, max_lat] for the area of interest.\n",
    "- YEAR: The year for which to analyze data.\n",
    "- TOP_N: The number of top anomalies to analyze with the LLM.\n",
    "- LLM_PROVIDER: Choose between 'openrouter' or 'openai'.\n",
    "- LLM_MODEL: Specify the model to use (e.g., 'google/gemma-3-27b-it' for OpenRouter, 'gpt-4o-mini' for OpenAI).\n",
    "- FORCE_REFRESH: Set to True to re-download all data, ignoring cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PIPELINE CONFIGURATION\n",
    "# ===============================================================================\n",
    "\n",
    "# Bounding Box for Amazonas, Brazil (min_lon, min_lat, max_lon, max_lat)\n",
    "BBOX_COORDS = [-59.813892, -5.253821, -58.332325, -3.983349]\n",
    "\n",
    "# Year for analysis\n",
    "YEAR = 2022\n",
    "\n",
    "# Number of top anomalies to send to LLM\n",
    "TOP_N = 3\n",
    "\n",
    "# LLM Configuration\n",
    "LLM_PROVIDER = 'openrouter' # 'openai' or 'openrouter'\n",
    "LLM_MODEL = 'google/gemma-3-27b-it' # Or 'gpt-4o-mini' etc.\n",
    "\n",
    "# Data Caching\n",
    "CACHE_DIR = \"data/raw\"\n",
    "FORCE_REFRESH = False # Set to True to re-download all data\n",
    "SHOW_IMAGES = True # Set to False to suppress matplotlib plots\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def make_bbox(min_lon, min_lat, max_lon, max_lat):\n",
    "    \"\"\"Construct a bounding box as a dict.\"\"\"\n",
    "    return {\n",
    "        'min_lon': min_lon, 'min_lat': min_lat,\n",
    "        'max_lon': max_lon, 'max_lat': max_lat\n",
    "    }\n",
    "\n",
    "# Create the bounding box object\n",
    "BBOX = make_bbox(*BBOX_COORDS)\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "Path(CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Configuration set:\")\n",
    "print(f\"BBOX: {BBOX}\")\n",
    "print(f\"YEAR: {YEAR}\")\n",
    "print(f\"TOP_N: {TOP_N}\")\n",
    "print(f\"LLM Provider: {LLM_PROVIDER}\")\n",
    "print(f\"LLM Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetchiing WorkFlow:\n",
    "\n",
    "All of the pertinent functions needed for the data fetching workflow:\n",
    "Functions for:\n",
    "- bbox to ee.Geometry, Shapely polygon functions.\n",
    "- functions for reusable requests session and generation of cache directories for the data.\n",
    "- functions for fetching LiDAR and Sentinel-2 data.\n",
    "- functions for fetching GEDI, PRODES, and SRTM data.\n",
    "- pipelines for efficient fetching and retrieval of the LiDAR/Sentinel-2 and GEDI/PRODES/SRTM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# BBOX FUNCTIONS\n",
    "# ===============================================================================\n",
    "def bbox_to_polygon(bbox):\n",
    "    \"\"\"Convert bbox dict to Shapely polygon.\"\"\"\n",
    "    return box(bbox['min_lon'], bbox['min_lat'], bbox['max_lon'], bbox['max_lat'])\n",
    "\n",
    "def bbox_to_ee_geometry(bbox):\n",
    "    \"\"\"Convert bbox dict to Earth Engine geometry.\"\"\"\n",
    "    return ee.Geometry.Rectangle([\n",
    "        bbox['min_lon'], bbox['min_lat'], bbox['max_lon'], bbox['max_lat']\n",
    "    ])\n",
    "\n",
    "# ===============================================================================\n",
    "# REQUESTS SESSION AND CACHE HELPERS\n",
    "# ===============================================================================\n",
    "def get_requests_session():\n",
    "    \"\"\"Create a reusable requests session with retry strategy.\"\"\"\n",
    "    session = requests.Session()  \n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "        backoff_factor=1\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    # Set a default User-Agent\n",
    "    session.headers.update({\"User-Agent\": \"openai-to-z/0.1 (Dataset fetcher)\"})\n",
    "    return session\n",
    "\n",
    "def ensure_cache_dir(cache_dir):\n",
    "    \"\"\"Ensure the cache directory exists, creating it if necessary.\"\"\"\n",
    "    cache_path = Path(cache_dir)\n",
    "    cache_path.mkdir(parents=True, exist_ok=True)\n",
    "    return cache_path\n",
    "\n",
    "def generate_cache_key(bbox, year, dataset):\n",
    "    \"\"\"\n",
    "    Generate a cache key based on bbox, year, and dataset.\n",
    "    Converts key data to a sorted json string, then returns a md5 hash of the string.\n",
    "    \"\"\"\n",
    "    key_data = {\n",
    "        \"bbox\": [bbox['min_lon'], bbox['min_lat'], bbox['max_lon'], bbox['max_lat']],\n",
    "        \"year\": year,\n",
    "        \"dataset\": dataset,\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    "    key_string = json.dumps(key_data, sort_keys=True)\n",
    "    return hashlib.md5(key_string.encode()).hexdigest()\n",
    "\n",
    "def get_cache_path(cache_dir, cache_key, dataset, ext=\".csv\"):\n",
    "    \"\"\"Get the full cache path for a given dataset and cache key.\"\"\"\n",
    "    return Path(cache_dir) / f\"{dataset}_{cache_key}{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# LIDAR DATA FETCHING (OpenTopography API)\n",
    "# ===============================================================================\n",
    "def fetch_lidar_ot_data(demtype: str, south: float, north: float, west: float, east: float, \n",
    "    api_key=OT_API_KEY) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Download a LiDAR .tif file from OpenTopography API.\n",
    "    Takes in the available global raster dataset type (demtype), the bbox coordinates,\n",
    "    and returns the path to the downloaded temporary file.\n",
    "    \"\"\"\n",
    "    url = \"https://portal.opentopography.org/API/globaldem\"\n",
    "    params = {\n",
    "        \"demtype\": demtype,\n",
    "        \"south\": south,\n",
    "        \"north\": north,\n",
    "        \"west\": west,\n",
    "        \"east\": east,\n",
    "        \"outputFormat\": \"GTiff\",\n",
    "        \"API_Key\": api_key\n",
    "    }\n",
    "    try:\n",
    "        tf = tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False)\n",
    "        resp = requests.get(url, params=params, timeout=120)\n",
    "        resp.raise_for_status()\n",
    "        tf.write(resp.content)\n",
    "        tf.close()\n",
    "        logger.info(f\"LiDAR data successfully downloaded to temporary file: {tf.name}\")\n",
    "        return tf.name\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading LiDAR data: {e}\")\n",
    "        return None\n",
    "    \n",
    "# ===============================================================================\n",
    "# SENTINEL-2 DATA FETCHING AND PROCESSING (Google Earth Engine)\n",
    "# ===============================================================================\n",
    "def cloud_mask_s2_sr(image: ee.Image) -> ee.Image:\n",
    "    \"\"\"Cloud mask creation for Sentinel-2 Surface Reflectance using SCL band.\"\"\"\n",
    "    scl = image.select('SCL')\n",
    "    # Mask out cloud shadow, medium/high probability cloud, and cirrus, saturated/defective pixels:\n",
    "    clear_mask = scl.neq(1).And(scl.neq(3)).And(scl.neq(8)).And(scl.neq(9)).And(scl.neq(10))\n",
    "    valid_data_mask = image.select('B2').gt(0)\n",
    "    return image.updateMask(clear_mask.And(valid_data_mask)).divide(10000) # Scale data to 0-1 range.\n",
    "\n",
    "def fetch_sentinel2_gee_data(\n",
    "    south: float, north: float, west: float, east: float, start_date: str, end_date: str,\n",
    "    max_cloud_percentage: float = 20.0\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetching Sentinel-2 L2A median composite from GEE (Copernicus/S2_SR_HARMONIZED) for a given bbox and date range.\n",
    "    Returns a dictionary containing the GEE image object and ROI.\n",
    "    \"\"\"\n",
    "    if not gee_initialized_successfully:\n",
    "        if not initialize_gee():\n",
    "            raise RuntimeError(\"GEE could not be initialized. Cannot fetch Sentinel-2 data.\")\n",
    "    roi = ee.Geometry.Rectangle([west, south, east, north])\n",
    "\n",
    "    # Filtering ImageCollection by ROI, date range, as well as cloud percentage:\n",
    "    s2_collection = (\n",
    "        ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "        .filterBounds(roi)\n",
    "        .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
    "        .filter(ee.Filter.lt(\"CLOUDY_PIXEL_PERCENTAGE\", max_cloud_percentage))\n",
    "    )\n",
    "    count = s2_collection.size().getInfo()\n",
    "    if count == 0:\n",
    "        logger.warning(f\"No Sentinel-2 images found for the given criteria in GEE.\")\n",
    "        logger.warning(f\"ROI: {west},{south},{east},{north}\")\n",
    "        logger.warning(f\"Date: {start_date} to {end_date}, Cloud %: < {max_cloud_percentage}\")\n",
    "        return {\"image\": None, \"roi\": roi, \"count\": 0, \"error\": \"No images found\"}\n",
    "    logger.info(f\"Found {count} Sentinel-2 images. Creating median composite...\")\n",
    "    \n",
    "    # All 12 multispectral bands in S2_SR_HARMONIZED... B10, which is used to detech cirrus clouds and used \n",
    "    # for atomospheric correction purposes in L1C data, is not included in L2A, since its already atmos corrected.\n",
    "    all_spectral_bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
    "    \n",
    "    # Creating median composite via applying cloud/valid mask, selecting defined bands, and clipping to ROI:\n",
    "    composite_image = (\n",
    "        s2_collection.map(cloud_mask_s2_sr)\n",
    "        .select(all_spectral_bands)\n",
    "        .median()\n",
    "        .clip(roi)\n",
    "    )\n",
    "    # Lets check if the median composite image is valid: if no band names, all pixels masked, thus empty. Not good.\n",
    "    try:\n",
    "        composite_image.bandNames().getInfo()\n",
    "    except ee.EEException as e:\n",
    "        logger.error(f\"Error creating valid GEE composite (likely all pixels masked): {e}\")\n",
    "        return {\"image\": None, \"roi\": roi, \"count\": count, \"error\": \"Composite image is empty/invalid.\"}\n",
    "    logger.info(f\"Sentinel-2 GEE median composite created for period {start_date} to {end_date}.\")\n",
    "    return {\n",
    "        \"image\": composite_image,\n",
    "        \"roi\": roi,\n",
    "        \"count\": count,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"roi_bounds\": [west, south, east, north],\n",
    "        \"bands_included\": all_spectral_bands\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SRTM DATA FETCHING AND PROCESSING (Google Earth Engine)\n",
    "# ==============================================================================\n",
    "def fetch_srtm(bbox, cache_dir=\"data/raw\", force_refresh=False, sample_scale=30):\n",
    "    \"\"\"\n",
    "    Fetch SRTM elevation data within the bounding box and compute slope metrics.\n",
    "    Returns a DataFrame with columns: lon, lat, elevation, slope_degrees, aspect_degrees.\n",
    "    Uses Google Earth Engine for data access. \n",
    "    \"\"\"\n",
    "    ensure_cache_dir(cache_dir)\n",
    "    cache_year = 2000 # SRTM is static, thus using a fixed 2000 year for caching purposes.\n",
    "    cache_key = generate_cache_key(bbox, cache_year, \"srtm\")\n",
    "    cache_path = get_cache_path(cache_dir, cache_key, \"srtm\")\n",
    "    if not force_refresh and cache_path.exists():\n",
    "        logger.info(f\"Loading SRTM data from cache: {cache_path}\")\n",
    "        try:\n",
    "            return pd.read_csv(cache_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load from cache: {e}\")\n",
    "    logger.info(f\"Fetching SRTM elevation and slope data via Google Earth Engine\")\n",
    "    try:\n",
    "        # Initialize Earth Engine if needed:\n",
    "        if not gee_initialized_successfully:\n",
    "            if not initialize_gee():\n",
    "                logger.error(\"Failed to initialize Google Earth Engine for SRTM data\")\n",
    "                return pd.DataFrame(columns=['lon', 'lat', 'elevation', 'slope_degrees', 'aspect_degrees'])\n",
    "        # Loading SRTM elevation data (30m), selecting elevation band, then computing slope and aspect in degrees:\n",
    "        srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "        elevation = srtm.select('elevation')\n",
    "        slope = ee.Terrain.slope(elevation)\n",
    "        aspect = ee.Terrain.aspect(elevation)\n",
    "        \n",
    "        # Combining elevation, slope, and aspect into a single image:\n",
    "        terrain_image = elevation.addBands([slope, aspect]).rename(['elevation', 'slope_degrees', 'aspect_degrees'])\n",
    "\n",
    "        # Computing the appropriate bbox area to determine the optimal number of pixels to sample:\n",
    "        bbox_geometry = bbox_to_ee_geometry(bbox)\n",
    "        area_km2 = bbox_geometry.area().divide(1000000).getInfo()  # Convert to km²\n",
    "        \n",
    "        # Conservative sampling to avoid GEE limits - use lower density for larger areas:\n",
    "        if area_km2 > 1000:  # Large area\n",
    "            target_density = 100\n",
    "        elif area_km2 > 100:  # Medium area\n",
    "            target_density = 300\n",
    "        else:  # Small area\n",
    "            target_density = 500\n",
    "            \n",
    "        optimal_pixels = int(area_km2 * target_density)\n",
    "        # Strict bounds to avoid GEE collection limits:\n",
    "        num_pixels = max(100, min(optimal_pixels, 3000))  # Much lower max limit\n",
    "        logger.info(f\"Sampling {num_pixels} pixels for the bbox area of {area_km2:.2f} km²\")\n",
    "        \n",
    "        samples = terrain_image.sample(\n",
    "            region=bbox_geometry,\n",
    "            scale=sample_scale,\n",
    "            numPixels=num_pixels,\n",
    "            seed=42,\n",
    "            geometries=True\n",
    "        )\n",
    "        # Checking if there are any raster samples found:\n",
    "        raster_data = samples.getInfo()\n",
    "        if not raster_data['features']:\n",
    "            logger.warning(\"No SRTM elevation data found for the specified bbox\")\n",
    "            return pd.DataFrame(columns=['lon', 'lat', 'elevation', 'slope_degrees', 'aspect_degrees'])\n",
    "        \n",
    "        # Constructing a records list from the raster_data features:\n",
    "        records = []\n",
    "        for feature in raster_data['features']:\n",
    "            props = feature['properties']\n",
    "            coords = feature['geometry']['coordinates']\n",
    "            record = {\n",
    "                'lon': coords[0],\n",
    "                'lat': coords[1],\n",
    "                'elevation': props.get('elevation'),\n",
    "                'slope_degrees': props.get('slope_degrees'),\n",
    "                'aspect_degrees': props.get('aspect_degrees')\n",
    "            }\n",
    "            records.append(record)\n",
    "        \n",
    "        # Constructing a pandas DataFrame from the records list, filtering out invalid coords, rasters outside bbox:\n",
    "        df = pd.DataFrame(records)\n",
    "        df = df.dropna(subset=['lon', 'lat', 'elevation'])\n",
    "        df = df[(df['lon'] >= bbox['min_lon']) & (df['lon'] <= bbox['max_lon'])]\n",
    "        df = df[(df['lat'] >= bbox['min_lat']) & (df['lat'] <= bbox['max_lat'])]\n",
    "        df = df.round(4) # Rounding values for consistency.\n",
    "        logger.info(f\"Fetched {len(df)} SRTM elevation points with terrain metrics\")\n",
    "        logger.info(f\"Elevation range: {df['elevation'].min():.1f} - {df['elevation'].max():.1f} m\")\n",
    "        logger.info(f\"Slope range: {df['slope_degrees'].min():.1f} - {df['slope_degrees'].max():.1f} degrees\")\n",
    "        \n",
    "        # Caching the DataFrame to a CSV file for further usage:\n",
    "        df.to_csv(cache_path, index=False)\n",
    "        logger.info(f\"Cached SRTM data to: {cache_path}\")\n",
    "        return df        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch SRTM data: {e}\")\n",
    "        return pd.DataFrame(columns=['lon', 'lat', 'elevation', 'slope_degrees', 'aspect_degrees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# GEDI DATA FETCHING AND PROCESSING (Google Earth Engine)\n",
    "# ==============================================================================\n",
    "def fetch_gedi(bbox, year, cache_dir=\"data/raw\", force_refresh=False):\n",
    "    \"\"\"\n",
    "    Fetch GEDI L2A vector shots within the bounding box for the specified year.\n",
    "    Uses Google Earth Engine for data access. Filters for quality_flag == 1\n",
    "    and removes shots with invalid coordinates or missing canopy height.\n",
    "    Returns DataFrame with columns: lon, lat, rh98, canopy_height, quality_flag, shot_number\n",
    "    Note: canopy_height is mapped from rh100, lat/lon from lat_highestreturn/lon_highestreturn\n",
    "    \"\"\"\n",
    "    ensure_cache_dir(cache_dir)\n",
    "    cache_key = generate_cache_key(bbox, year, \"gedi\")\n",
    "    cache_path = get_cache_path(cache_dir, cache_key, \"gedi\")\n",
    "    \n",
    "    if not force_refresh and cache_path.exists():\n",
    "        logger.info(f\"Loading GEDI data from cache: {cache_path}\")\n",
    "        try:\n",
    "            return pd.read_csv(cache_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load from cache: {e}\")\n",
    "    \n",
    "    logger.info(f\"Fetching GEDI L2A vector data for year {year} via Google Earth Engine\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize Earth Engine if needed:\n",
    "        if not gee_initialized_successfully:\n",
    "            if not initialize_gee():\n",
    "                logger.error(\"Failed to initialize Google Earth Engine for GEDI data\")\n",
    "                return pd.DataFrame(columns=['lon', 'lat', 'rh98', 'canopy_height', 'quality_flag', 'shot_number'])\n",
    "        \n",
    "        # Usage of GEDI L2A table index to find available data tables; creating date filter for the specified year:\n",
    "        gedi_index = ee.FeatureCollection(\"LARSE/GEDI/GEDI02_A_002_INDEX\")\n",
    "        start_date = f\"{year}-01-01\"\n",
    "        end_date = f\"{year + 1}-01-01\"\n",
    "        \n",
    "        # Filtering the index by date and bounds to find relevant tables, then getting the list of table IDs:\n",
    "        bbox_geometry = bbox_to_ee_geometry(bbox)\n",
    "        relevant_tables = (gedi_index\n",
    "                          .filter(ee.Filter.And(\n",
    "                              ee.Filter.gte('time_start', start_date),\n",
    "                              ee.Filter.lte('time_end', end_date)\n",
    "                          ))\n",
    "                          .filterBounds(bbox_geometry))\n",
    "        table_info = relevant_tables.getInfo()\n",
    "        if not table_info['features']:\n",
    "            logger.warning(\"No GEDI tables found for the specified bbox and year\")\n",
    "            return pd.DataFrame(columns=['lon', 'lat', 'rh98', 'canopy_height', 'quality_flag', 'shot_number'])\n",
    "        logger.info(f\"Found {len(table_info['features'])} GEDI tables for the specified period\")\n",
    "        \n",
    "        # Computing the optimal sampling based on bbox area:\n",
    "        area_km2 = bbox_geometry.area().divide(1000000).getInfo()\n",
    "        target_density = 1000\n",
    "        optimal_samples = int(area_km2 * target_density)\n",
    "        max_samples = min(optimal_samples, 50000)  # GEE limit consideration:\n",
    "        samples_per_table = max(100, max_samples // len(table_info['features']))\n",
    "        logger.info(f\"Sampling up to {samples_per_table} shots per table for bbox area of {area_km2:.2f} km²\")\n",
    "        \n",
    "        # Collecting data from all relevant tables:\n",
    "        all_records = []\n",
    "        for table_feature in table_info['features']:\n",
    "            table_id = table_feature['properties']['table_id']\n",
    "            try:\n",
    "                # Loading the specific GEDI table (table_id already includes full path):\n",
    "                gedi_table = ee.FeatureCollection(table_id)\n",
    "                # Filtering by bounds and quality:\n",
    "                gedi_filtered = (gedi_table\n",
    "                               .filterBounds(bbox_geometry)\n",
    "                               .filter(ee.Filter.eq('quality_flag', 1)))\n",
    "                \n",
    "                # Selecting only the fields we need:\n",
    "                gedi_selected = gedi_filtered.select([\n",
    "                    'lat_highestreturn', 'lon_highestreturn',  # Correct coordinate fields.\n",
    "                    'rh98', 'rh100', 'quality_flag', 'shot_number'\n",
    "                ])\n",
    "                \n",
    "                # Sampling the table if it's too large:\n",
    "                table_size = gedi_selected.size().getInfo()\n",
    "                if table_size > samples_per_table:\n",
    "                    gedi_sampled = gedi_selected.randomColumn('random').sort('random').limit(samples_per_table)\n",
    "                else:\n",
    "                    gedi_sampled = gedi_selected\n",
    "                \n",
    "                # Getting the data:\n",
    "                table_data = gedi_sampled.getInfo()\n",
    "                if table_data['features']:\n",
    "                    all_records.extend(table_data['features'])\n",
    "                    logger.info(f\"Collected {len(table_data['features'])} shots from table {table_id}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to process table {table_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_records:\n",
    "            logger.warning(\"No GEDI shots found after processing all tables\")\n",
    "            return pd.DataFrame(columns=['lon', 'lat', 'rh98', 'canopy_height', 'quality_flag', 'shot_number'])\n",
    "        logger.info(f\"Total collected: {len(all_records)} GEDI shots from {len(table_info['features'])} tables\")\n",
    "        \n",
    "        # Extracting records from features:\n",
    "        records = []\n",
    "        for feature in all_records:\n",
    "            props = feature['properties']\n",
    "            \n",
    "            # Skipping if any critical values are null:\n",
    "            if (props.get('rh98') is None or \n",
    "                props.get('rh100') is None or \n",
    "                props.get('lat_highestreturn') is None or \n",
    "                props.get('lon_highestreturn') is None):\n",
    "                continue\n",
    "                \n",
    "            record = {\n",
    "                'lon': props.get('lon_highestreturn'),  # Map from lon_highestreturn\n",
    "                'lat': props.get('lat_highestreturn'),  # Map from lat_highestreturn\n",
    "                'rh98': props.get('rh98'),\n",
    "                'canopy_height': props.get('rh100'),  # Map rh100 to canopy_height for compatibility\n",
    "                'quality_flag': props.get('quality_flag', 1),\n",
    "                'shot_number': props.get('shot_number')\n",
    "            }\n",
    "            records.append(record)\n",
    "        \n",
    "        # Creating a DataFrame and filtering:\n",
    "        df = pd.DataFrame(records)\n",
    "        if df.empty:\n",
    "            logger.warning(\"No valid GEDI records after filtering\")\n",
    "            return df\n",
    "            \n",
    "        # Additional spatial filtering and data cleaning:\n",
    "        df = df.dropna(subset=['lon', 'lat', 'rh98', 'canopy_height'])\n",
    "        df = df[(df['lon'] >= bbox['min_lon']) & (df['lon'] <= bbox['max_lon'])]\n",
    "        df = df[(df['lat'] >= bbox['min_lat']) & (df['lat'] <= bbox['max_lat'])]\n",
    "        \n",
    "        logger.info(f\"Fetched {len(df)} quality GEDI shots\")\n",
    "        \n",
    "        # Caching the results:\n",
    "        df.to_csv(cache_path, index=False)\n",
    "        logger.info(f\"Cached GEDI data to: {cache_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch GEDI data: {e}\")\n",
    "        return pd.DataFrame(columns=['lon', 'lat', 'rh98', 'canopy_height', 'quality_flag', 'shot_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PRODES DATA FETCHING AND PROCESSING (TerraBrasilis API)\n",
    "# ==============================================================================\n",
    "def fetch_prodes(bbox, year, cache_dir=\"data/raw\", force_refresh=False):\n",
    "    \"\"\"\n",
    "    Fetch PRODES deforestation polygons from TerraBrasilis.\n",
    "\n",
    "    This function automatically downloads and extracts the source GeoPackage file if it's\n",
    "    not found locally. It then filters the data for the specified year and bounding\n",
    "    box, caching the final filtered result as a GeoJSON for fast subsequent access.\n",
    "    \"\"\"\n",
    "    ensure_cache_dir(cache_dir)\n",
    "    cache_key = generate_cache_key(bbox, year, \"prodes\")\n",
    "    cache_path = get_cache_path(cache_dir, cache_key, \"prodes\", \".geojson\")\n",
    "\n",
    "    if not force_refresh and cache_path.exists():\n",
    "        logger.info(f\"Loading PRODES data from cache: {cache_path}\")\n",
    "        try:\n",
    "            return gpd.read_file(cache_path)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load from cache: {e}\")\n",
    "\n",
    "    # Define paths for the source GeoPackage and its zipped version\n",
    "    geopackage_path = Path(cache_dir) / \"prodes_amazonia_nb.gpkg\"\n",
    "    zip_url = \"https://terrabrasilis.dpi.inpe.br/download/dataset/amz-prodes/vector/prodes_amazonia_nb.gpkg.zip\"\n",
    "    zip_path = Path(cache_dir) / \"prodes_amazonia_nb.gpkg.zip\"\n",
    "\n",
    "    # --- New Download & Unzip Logic ---\n",
    "    # If the main GeoPackage file doesn't exist, download and extract it.\n",
    "    if not geopackage_path.exists():\n",
    "        logger.info(f\"Source PRODES GeoPackage not found at: {geopackage_path}\")\n",
    "        logger.info(f\"Downloading from {zip_url}...\")\n",
    "        try:\n",
    "            # Download the file with a timeout\n",
    "            with requests.get(zip_url, stream=True, timeout=300) as r: # 5-minute timeout\n",
    "                r.raise_for_status()\n",
    "                with open(zip_path, 'wb') as f:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "            logger.info(\"Download complete.\")\n",
    "\n",
    "            # Unzip the file\n",
    "            logger.info(f\"Extracting {zip_path}...\")\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(path=cache_dir)\n",
    "            logger.info(f\"Successfully extracted GeoPackage to {cache_dir}\")\n",
    "\n",
    "            # Clean up the downloaded zip file to save space\n",
    "            os.remove(zip_path)\n",
    "            logger.info(f\"Removed temporary zip file: {zip_path}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Failed to download PRODES data: {e}\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])\n",
    "        except zipfile.BadZipFile as e:\n",
    "            logger.error(f\"Failed to extract PRODES GeoPackage from zip: {e}\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred during PRODES data setup: {e}\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])\n",
    "    # --- End of New Logic ---\n",
    "\n",
    "    if not geopackage_path.exists():\n",
    "        logger.error(f\"PRODES GeoPackage not found at: {geopackage_path}\")\n",
    "        return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Loading GeoPackage from: {geopackage_path}\")\n",
    "        # Load the yearly deforestation layer specifically\n",
    "        layer_name = 'yearly_deforestation_biome'\n",
    "        gdf = gpd.read_file(geopackage_path, layer=layer_name)\n",
    "        logger.info(f\"Loaded GeoPackage layer '{layer_name}' with {len(gdf)} total polygons\")\n",
    "        \n",
    "        # Filter by year\n",
    "        gdf_year = gdf[gdf['year'] == year]\n",
    "        logger.info(f\"Filtered to {len(gdf_year)} polygons for year {year}\")\n",
    "\n",
    "        if gdf_year.empty:\n",
    "            logger.warning(f\"No PRODES polygons found for year {year}\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])\n",
    "\n",
    "        # Create bbox polygon for spatial filtering and clip\n",
    "        bbox_polygon = bbox_to_polygon(bbox)\n",
    "        gdf_clipped = gdf_year[gdf_year.geometry.intersects(bbox_polygon)]\n",
    "        logger.info(f\"After bbox filtering: {len(gdf_clipped)} polygons\")\n",
    "\n",
    "        if gdf_clipped.empty:\n",
    "            logger.warning(f\"No PRODES polygons found within bbox {bbox} for year {year}\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])\n",
    "\n",
    "        # Select and prepare final columns\n",
    "        gdf_result = gdf_clipped[['geometry', 'year', 'area_ha']].copy()\n",
    "\n",
    "        # Cache the filtered result as GeoJSON\n",
    "        gdf_result.to_file(cache_path, driver='GeoJSON')\n",
    "        logger.info(f\"Fetched {len(gdf_result)} PRODES polygons covering {gdf_result['area_ha'].sum():.1f} hectares\")\n",
    "        logger.info(f\"Cached filtered PRODES data to: {cache_path}\")\n",
    "\n",
    "        return gdf_result\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load or process PRODES data from GeoPackage: {e}\")\n",
    "        return gpd.GeoDataFrame(columns=['geometry', 'year', 'area_ha'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FETCH PIPELINE FOR GEDI, PRODES, and SRTM\n",
    "# ==============================================================================\n",
    "def gedi_prodes_srtm_fetch_pipeline(bbox: dict = None, year: int = 2022, cache_dir=\"data/raw\", force_refresh: bool = False):\n",
    "    \"\"\"\n",
    "    Fetching pipeline for the GEDI, PRODES, and SRTM datasets for the specified bbox and year.\n",
    "    If bbox and/or year is None, uses the default coordinates and/or year.\n",
    "    Returns a tuple of the GEDI and SRTM DataFrames, as well as the PRODES GeoDataFrame. \n",
    "    force_refresh determines whether to force a refresh of the cached data. Defaults to 'False'.\n",
    "    GEDI data comes from 'GEDI L2A Raster Canopy Top Height (Version 2, Monthly)', via Google Earth Engine (GEE).\n",
    "        - https://developers.google.com/earth-engine/datasets/catalog/LARSE_GEDI_GEDI02_A_002_MONTHLY#description\n",
    "    PRODES data comes from 'TerraBrasilis Native Vegetation Suppression Map (PRODES)', loaded from local GeoPackage file.\n",
    "        - Source: https://terrabrasilis.dpi.inpe.br/app/map/deforestation?hl=en\n",
    "    SRTM data comes from 'NASA SRTM Digital Elevation V3', via Google Earth Engine (GEE).\n",
    "        - https://developers.google.com/earth-engine/datasets/catalog/USGS_SRTMGL1_003\n",
    "    \"\"\"\n",
    "    # Use default coordinates, created via make_box function for easy plugin; useful for test runs:\n",
    "    if bbox is None:\n",
    "        bbox = make_bbox(S2_DEFAULT_WEST, S2_DEFAULT_SOUTH, S2_DEFAULT_EAST, S2_DEFAULT_NORTH)\n",
    "    # If bbox is already a proper dict with the required keys, use it as-is\n",
    "    # Otherwise, assume it needs to be properly formatted\n",
    "    \n",
    "    # Retrieving the GEDI data; if there is no data for the specified bbox/year, function will return empty DataFrame:\n",
    "    logger.info(f\"FETCHING GEDI L2A RASTER CANOPY TOP HEIGHT V2 DATA FROM GEE FOR THE FOLLOWING BBOX:\\n{bbox}\\nYEAR:{year}\\n\")\n",
    "    gedi_df = fetch_gedi(bbox, year, cache_dir, force_refresh)\n",
    "    if gedi_df.empty is True:\n",
    "        logger.warning(\"FETCH DID NOT RETURN ANY DATA. THERE WAS AN ERROR IN THE FETCHING PROCESS OR NO DATA AVAILABLE \\\n",
    "            FOR THE SPECIFIED BBOX AND YEAR.\\n\")\n",
    "\n",
    "    # Retrieving the PRODES data; if there is no data for the specified bbox/year, function will return empty DataFrame:\n",
    "    logger.info(f\"FETCHING PRODES DATA FROM LOCAL GEOPACKAGE FOR THE FOLLOWING BBOX:\\n{bbox}\\nYEAR:{year}\\n\")\n",
    "    prodes_gdf = fetch_prodes(bbox, year, cache_dir, force_refresh)\n",
    "    if prodes_gdf.empty is True:\n",
    "        logger.warning(\"FETCH DID NOT RETURN ANY DATA. THERE WAS AN ERROR IN THE FETCHING PROCESS OR NO DATA AVAILABLE \\\n",
    "            FOR THE SPECIFIED BBOX AND YEAR.\\n\")\n",
    "\n",
    "    # Retrieving the SRTM data; if there is no data for the specified bbox/year, function will return empty DataFrame:\n",
    "    logger.info(f\"FETCHING SRTM DATA FROM GEE FOR THE FOLLOWING BBOX:\\n{bbox}\\n\")\n",
    "    srtm_df = fetch_srtm(bbox, cache_dir, force_refresh)\n",
    "    if srtm_df.empty is True:\n",
    "        logger.warning(\"FETCH DID NOT RETURN ANY DATA. THERE WAS AN ERROR IN THE FETCHING PROCESS OR NO DATA AVAILABLE \\\n",
    "            FOR THE SPECIFIED BBOX AND YEAR.\\n\")\n",
    "\n",
    "    # Returning some preliminary data for debugging, informational purposes:\n",
    "    logger.info(f\"GEDI_PRODES_SRTM_FETCH_PIPELINE COMPLETED. RETURNING THE FOLLOWING DATA:\\n\\\n",
    "        GEDI Shots: {len(gedi_df)}\\n\\\n",
    "        PRODES Polygons: {len(prodes_gdf)}\\n\\\n",
    "        SRTM Points: {len(srtm_df)}\")\n",
    "    \n",
    "    # Finally, returning the LiDAR and Sentinel-2 data for further processing and usage:\n",
    "    return gedi_df, prodes_gdf, srtm_df\n",
    "\n",
    "# ==============================================================================\n",
    "# FETCHING PIPELINE FOR LIDAR AND SENTINEL-2\n",
    "# ==============================================================================\n",
    "def lidar_sentinel2_fetch_pipeline(bbox: dict = None) -> dict:\n",
    "    \"\"\"\n",
    "    Fetching pipeline for the LiDAR and Sentinel-2 datasets, given a specified bounding box (bbox).\n",
    "    If bbox is None, uses the default coordinates.\n",
    "    Returns a dict with the LiDAR file path (GeoTIFF data) and the Sentinel-2 GEE median composite image and ROI.\n",
    "    LiDAR data comes from 'Copernicus Global Digital Elevation Models' (COP30), via OpenTopography API.\n",
    "        - https://portal.opentopography.org/datasetMetadata?otCollectionID=OT.032021.4326.1  \n",
    "    Sentinel-2 data comes from 'Copernicus Harmonnized Sentinel-2 MSI: MultiSpectral Instrument, Level-2A (SR)', via Google Earth Engine (GEE).\n",
    "        - https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_SR_HARMONIZED\n",
    "    \"\"\"\n",
    "    logger.info(f\"FETCHING LIDAR DATA FROM COP30 (OpenTopography API) FOR THE FOLLOWING BBOX:\\n{bbox}\\n\")\n",
    "    # Use default coordinates; useful for test runs:\n",
    "    if bbox is None:\n",
    "        south, north = S2_DEFAULT_SOUTH, S2_DEFAULT_NORTH\n",
    "        west, east = S2_DEFAULT_WEST, S2_DEFAULT_EAST\n",
    "    # Use the specified bbox coordinates:    \n",
    "    else:\n",
    "        south, north = bbox['min_lat'], bbox['max_lat']\n",
    "        west, east = bbox['min_lon'], bbox['max_lon']\n",
    "        \n",
    "    # Retrieving the LiDAR data; if there is no data for the specified bbox, set to None but continue.\n",
    "    lidar_path = fetch_lidar_ot_data(demtype=\"COP30\", south=south, north=north, west=west, east=east)\n",
    "    \n",
    "    # Retrieving the Sentinel-2 data; if there is no data for the specified bbox, set to None but continue:\n",
    "    print(f\"FETCHING SENTINEL-2 COMPOSITE DATA FROM COPERNICUS S2 SR HARMONIZED (GEE) FOR THE FOLLOWING BBOX:\\n{bbox}\\n\")\n",
    "    s2_data = fetch_sentinel2_gee_data(\n",
    "            south=south, north=north, west=west, east=east,\n",
    "            start_date=S2_DEFAULT_START_DATE, end_date=S2_DEFAULT_END_DATE\n",
    "        )\n",
    "    \n",
    "    # Only return None if both data sources failed\n",
    "    if lidar_path is None and s2_data is None:\n",
    "        logger.warning(\"Both LiDAR and Sentinel-2 data fetching failed\")\n",
    "        return None\n",
    "\n",
    "    # Returning some preliminary data for debugging, informational purposes: \n",
    "    logger.info(f\"LIDAR and SENTINEL-2 FETCHING COMPLETED. RETURNING THE FOLLOWING DATA:\\n\\\n",
    "        LiDAR Path: {lidar_path}\\n\\\n",
    "        Sentinel-2 Data: {s2_data}\")\n",
    "    \n",
    "    # Finally, returning the LiDAR and Sentinel-2 data for further processing and usage:\n",
    "    return {\n",
    "        'lidar_path': lidar_path,\n",
    "        's2_data': s2_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Workflow:\n",
    "\n",
    "Generate and displays plots of the LiDAR data, as well as some LiDAR stats. Then, generation of band and NDVI stats for the Sentinel-2 data, then generation of RGB, NDVI and false-color thumbnails for displaying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# LiDAR FEATURE EXTRACTION: OpenTopography API\n",
    "# ===============================================================================\n",
    "def lidar_ot_extract_features(lidar_path: str, show_image: bool = True) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate and display a plots of LiDAR GeoTIFF data along with some stats.\n",
    "    Returns a dict containing the plot as a BytesIO obj, and the stats.\n",
    "    Cleans up the temporary lidar_path file upon completion or error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(lidar_path) as src:\n",
    "            print(\"LiDAR data read.\")\n",
    "            lidar_arr = src.read(1).astype(np.float32)\n",
    "            if src.nodata is not None:\n",
    "                lidar_arr = np.where(lidar_arr == src.nodata, np.nan, lidar_arr)\n",
    "            if np.all(np.isnan(lidar_arr)):\n",
    "                print(\"Error: LiDAR data is empty or all NoData values.\")\n",
    "                return None\n",
    "            \n",
    "            # Usage of 2-98 percentiles for avoiding outliers that may affect color mapping:\n",
    "            vmin, vmax = np.nanpercentile(lidar_arr, [2, 98])\n",
    "\n",
    "            # Figure with subplots:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "            fig.suptitle(\"LiDAR Data Analysis\", fontsize=16)\n",
    "\n",
    "            # Main elevation plot:\n",
    "            print(\"Generating LiDAR elevation plot...\")\n",
    "            im1 = ax1.imshow(lidar_arr, cmap='terrain', vmin=vmin, vmax=vmax)\n",
    "            plt.colorbar(im1, ax=ax1, label='Elevation (m)')\n",
    "            ax1.set_title(\"LiDAR Elevation Data (2-98% range)\")\n",
    "\n",
    "            # Hillshade for better terrain visualization, using LightSource for azimuth and altitude:\n",
    "            print(\"Generating LiDAR hillshade...\")\n",
    "            ls = LightSource(azdeg=315, altdeg=45)\n",
    "            hillshade = ls.hillshade(lidar_arr, vert_exag=1, dx=src.res[0], dy=src.res[1], fraction=1.0)\n",
    "            ax2.imshow(hillshade, cmap='gray', alpha=0.8)\n",
    "            ax2.set_title(\"Hillshade Visualization\")\n",
    "            \n",
    "            # Show the plot if show_image is True:\n",
    "            print(\"Displaying the plot(s)...\")\n",
    "            if show_image:\n",
    "                plt.show()\n",
    "\n",
    "            # Save plot to BytesIO buffer as JPEG, tight bbox, 150 DPI:\n",
    "            print(\"Saving plot to BytesIO buffer...\")\n",
    "            buf = io.BytesIO()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(buf, format=\"JPEG\", bbox_inches=\"tight\", dpi=150)\n",
    "            plt.close()\n",
    "            buf.seek(0)\n",
    "            \n",
    "            # Binary to base64 conversion:\n",
    "            print(\"Converting plot to base64 and computation of stats...\")\n",
    "            image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "\n",
    "            # Compiling results:\n",
    "            ot_stats = {\n",
    "                \"image\": image_base64,\n",
    "                \"statistics\": {\n",
    "                    \"mean\": float(np.nanmean(lidar_arr)),\n",
    "                    \"median\": float(np.nanmedian(lidar_arr)),\n",
    "                    \"std\": float(np.nanstd(lidar_arr)),\n",
    "                    \"min\": float(np.nanmin(lidar_arr)),\n",
    "                    \"max\": float(np.nanmax(lidar_arr)),\n",
    "                    \"percentile_2\": float(vmin),\n",
    "                    \"percentile_98\": float(vmax),\n",
    "                    \"shape\": lidar_arr.shape\n",
    "                },\n",
    "                # Some spatial metadata: coordinate reference system, and bbox info:\n",
    "                \"crs\": str(src.crs) if src.crs else \"N/A\",\n",
    "                \"bounds\": list(src.bounds)\n",
    "            }\n",
    "            buf.close()\n",
    "            return ot_stats\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing LiDAR data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up the temporary file:\n",
    "        if os.path.exists(lidar_path):\n",
    "            os.unlink(lidar_path)\n",
    "            print(f\"Temporary file {lidar_path} deleted.\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# Sentinel-2 FEATURE EXTRACTION: Google Earth Engine\n",
    "# ===============================================================================\n",
    "def sentinel2_gee_extract_features(\n",
    "    gee_data: dict,\n",
    "    scale: int = 20, #Compromised resolution for all multispectral bands.\n",
    "    thumb_dimensions: str = '768x768', # Thumbnail dimensions.\n",
    "    show_image: bool = True\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Computing stats for all 12 multispectral bands and generating multiple visualizations for a \n",
    "    Sentinel-2 GEE Image. \n",
    "    Reminder: B10 is not included in the GEE Image being used here, due to it not being included in \n",
    "    the L2A data, since its already atmospherically corrected.\n",
    "    Returns a dict containing RGB composite, NDVI heatmap, false-color composite as base64,\n",
    "    along with comprehensive statistics and metadata for future AI analysis.\n",
    "    \"\"\"\n",
    "    if not gee_data or not gee_data.get(\"image\"):\n",
    "        error_msg = gee_data.get(\"error\", \"GEE image not provided or invalid.\")\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        return {\n",
    "            \"rgb_image\": None,\n",
    "            \"statistics\": {\"error\": error_msg},\n",
    "            \"roi_bounds\": gee_data.get(\"roi_bounds\"),\n",
    "            \"gee_image_details\": \"No image processed.\"\n",
    "        }\n",
    "\n",
    "    image = gee_data[\"image\"]\n",
    "    roi = gee_data[\"roi\"]\n",
    "    roi_bounds = gee_data[\"roi_bounds\"]\n",
    "\n",
    "    # Creating a composite reducer that computes multiple stats all in one operation:\n",
    "    reducers = (\n",
    "        ee.Reducer.mean().unweighted()\n",
    "        .combine(ee.Reducer.minMax().unweighted(), sharedInputs=True)\n",
    "        .combine(ee.Reducer.stdDev().unweighted(), sharedInputs=True)\n",
    "        .combine(ee.Reducer.percentile([2, 98]).unweighted(), sharedInputs=True)\n",
    "        .combine(ee.Reducer.count().unweighted(), sharedInputs=True)\n",
    "    )\n",
    "    # All 12 pertinent multispectral bands from Sentinel-2 will be used for full coverage:\n",
    "    all_spectral_bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']    \n",
    "    try:\n",
    "        # Computation of the stats for each selected band, given image, ROI, scale, and reducers:\n",
    "        print(f\"Calculating stats for bands: {all_spectral_bands} at {scale}m scale...\")\n",
    "        band_stats = image.select(all_spectral_bands).reduceRegion( \n",
    "            reducer=reducers,\n",
    "            geometry=roi,\n",
    "            scale=scale,\n",
    "            maxPixels=1e10,\n",
    "            tileScale=4\n",
    "        ).getInfo()\n",
    "        print(\"Band stats received.\")\n",
    "    except ee.EEException as e:\n",
    "        print(f\"GEE Error calculating band stats: {e}\")\n",
    "        return {\n",
    "            \"rgb_image\": None, \n",
    "            \"statistics\": {\"error\": f\"GEE band stats error: {e}\"},\n",
    "            \"roi_bounds\": roi_bounds,\n",
    "            \"gee_image_details\": \"Failed during band statistics.\"\n",
    "        }\n",
    "    except Exception as e_gen:\n",
    "        print(f\"General Error calculating band stats: {e_gen}\")\n",
    "        return {\n",
    "            \"rgb_image\": None, \n",
    "            \"statistics\": {\"error\": f\"General band stats error: {e_gen}\"},\n",
    "            \"roi_bounds\": roi_bounds,\n",
    "            \"gee_image_details\": \"Failed during band statistics.\"\n",
    "        }\n",
    "\n",
    "    # Calculating NDVI (Normalized Difference Vegetation Index) and its stats:\n",
    "    # NDVI is computed as (NIR - Red) / (NIR + Red), where NIR=near-infrared, band 8, and Red is band 4.\n",
    "    ndvi_stats = {}\n",
    "    try:\n",
    "        print(\"Computing Normalized Difference Vegetation Index (NDVI)...\")\n",
    "        ndvi_image = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "        ndvi_stats_raw = ndvi_image.reduceRegion(\n",
    "            reducer=reducers,\n",
    "            geometry=roi,\n",
    "            scale=scale,\n",
    "            maxPixels=1e10,\n",
    "            tileScale=4\n",
    "        ).getInfo()\n",
    "        print(\"NDVI stats received.\")\n",
    "        for key, value in ndvi_stats_raw.items():\n",
    "            ndvi_stats[f\"NDVI_{key}\"] = value\n",
    "    except ee.EEException as e:\n",
    "        print(f\"GEE Error calculating NDVI stats: {e}\")\n",
    "    except Exception as e_gen:\n",
    "        print(f\"General Error calculating NDVI stats: {e_gen}\")\n",
    "\n",
    "    # Merging the stats from all bands and NDVI into a single dict for usage:\n",
    "    all_stats = {}\n",
    "    for band in all_spectral_bands:\n",
    "        for stat_key, reducer_key_part in {\n",
    "            \"mean\": \"mean\", \"min\": \"min\", \"max\": \"max\", \"std\": \"stdDev\", \n",
    "            \"p2\": \"p2\", \"p98\": \"p98\", \"count\": \"count\" }.items():\n",
    "            gee_key = f\"{band}_{reducer_key_part}\"\n",
    "            all_stats[f\"{band}_{stat_key}\"] = band_stats.get(gee_key)\n",
    "    all_stats.update(ndvi_stats)\n",
    "\n",
    "    # Initializing variables for the image thumbnails:\n",
    "    rgb_image_base64 = None\n",
    "    ndvi_image_base64 = None\n",
    "    false_color_image_base64 = None\n",
    "    rgb_pil_image = None\n",
    "    ndvi_pil_image = None\n",
    "    false_color_pil_image = None\n",
    "\n",
    "    # Generating the RGB composite thumbnail for LLM processing:\n",
    "    try:\n",
    "        print(\"Generating RGB composite thumbnail...\")\n",
    "        # Vis params for 0-1 scaled reflectance. Common S2 vis is 0-0.3 range.\n",
    "        rgb_vis_params = {'bands': ['B4', 'B3', 'B2'], 'min': 0.0, 'max': 0.3, 'gamma': 1.4}\n",
    "        region_payload = roi.getInfo()['coordinates'] if hasattr(roi, 'getInfo') else roi\n",
    "        rgb_thumbnail_url = image.visualize(**rgb_vis_params).getThumbURL({\n",
    "            'region': region_payload,\n",
    "            'dimensions': thumb_dimensions,\n",
    "            'format': 'jpg'\n",
    "        })\n",
    "\n",
    "        # Downloading and processing the thumbnail; binary to base64 conversion:\n",
    "        print(f\"RGB composite thumbnail URL (first 100 chars): {rgb_thumbnail_url[:100]}...\")\n",
    "        with urllib.request.urlopen(rgb_thumbnail_url, timeout=60) as response:\n",
    "            img_data = response.read()\n",
    "        rgb_pil_image = Image.open(io.BytesIO(img_data))\n",
    "        buf = io.BytesIO()\n",
    "        rgb_pil_image.save(buf, format=\"JPEG\")\n",
    "        buf.seek(0)\n",
    "        rgb_image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "        buf.close()\n",
    "        print(\"RGB composite thumbnail generated and encoded.\")\n",
    "\n",
    "    except ee.EEException as e:\n",
    "        print(f\"GEE Error generating RGB composite thumbnail: {e}\")\n",
    "    except urllib.error.URLError as e_url:\n",
    "        print(f\"URL Error downloading RGB composite thumbnail: {e_url}\")\n",
    "    except Exception as e_thumb:\n",
    "        print(f\"General error generating RGB composite thumbnail: {e_thumb}\")\n",
    "\n",
    "    # Generating NDVI heatmap visualization:\n",
    "    try:\n",
    "        print(\"Generating NDVI heatmap visualization...\")\n",
    "        # NDVI visualization parameters: red-yellow-green scale for vegetation health\n",
    "        # Green/yellow for healthy vegetation (0.3-1.0), red/brown for bare soil/stress (-1.0-0.3)\n",
    "        ndvi_vis_params = {\n",
    "            'bands': ['NDVI'], \n",
    "            'min': -0.2, \n",
    "            'max': 0.8, \n",
    "            'palette': ['8B4513', 'CD853F', 'DEB887', 'F0E68C', 'ADFF2F', '32CD32', '228B22']  # Brown to green\n",
    "        }\n",
    "        \n",
    "        ndvi_thumbnail_url = ndvi_image.visualize(**ndvi_vis_params).getThumbURL({\n",
    "            'region': region_payload,\n",
    "            'dimensions': thumb_dimensions,\n",
    "            'format': 'jpg'\n",
    "        })\n",
    "        \n",
    "        print(f\"NDVI thumbnail URL (first 100 chars): {ndvi_thumbnail_url[:100]}...\")\n",
    "        with urllib.request.urlopen(ndvi_thumbnail_url, timeout=60) as response:\n",
    "            ndvi_img_data = response.read()\n",
    "        ndvi_pil_image = Image.open(io.BytesIO(ndvi_img_data))\n",
    "        ndvi_buf = io.BytesIO()\n",
    "        ndvi_pil_image.save(ndvi_buf, format=\"JPEG\")\n",
    "        ndvi_buf.seek(0)\n",
    "        ndvi_image_base64 = base64.b64encode(ndvi_buf.getvalue()).decode('utf-8')\n",
    "        ndvi_buf.close()\n",
    "        print(\"NDVI heatmap generated and encoded.\")\n",
    "        \n",
    "    except ee.EEException as e:\n",
    "        print(f\"GEE Error generating NDVI heatmap: {e}\")\n",
    "        ndvi_image_base64 = None\n",
    "    except urllib.error.URLError as e_url:\n",
    "        print(f\"URL Error downloading NDVI heatmap: {e_url}\")\n",
    "        ndvi_image_base64 = None\n",
    "    except Exception as e_ndvi:\n",
    "        print(f\"General error generating NDVI heatmap: {e_ndvi}\")\n",
    "        ndvi_image_base64 = None\n",
    "\n",
    "    # Generating false-color composite (NIR-Red-Green):\n",
    "    try:\n",
    "        print(\"Generating false-color composite (NIR-Red-Green)...\")\n",
    "        # False-color visualization: NIR->Red, Red->Green, Green->Blue\n",
    "        # Vegetation appears red/pink, water appears blue/black, urban appears cyan/blue\n",
    "        false_color_vis_params = {\n",
    "            'bands': ['B8', 'B4', 'B3'],  # NIR, Red, Green mapped to RGB\n",
    "            'min': 0.0, \n",
    "            'max': 0.3, \n",
    "            'gamma': 1.2\n",
    "        }\n",
    "        \n",
    "        false_color_thumbnail_url = image.visualize(**false_color_vis_params).getThumbURL({\n",
    "            'region': region_payload,\n",
    "            'dimensions': thumb_dimensions,\n",
    "            'format': 'jpg'\n",
    "        })\n",
    "        \n",
    "        print(f\"False-color thumbnail URL (first 100 chars): {false_color_thumbnail_url[:100]}...\")\n",
    "        with urllib.request.urlopen(false_color_thumbnail_url, timeout=60) as response:\n",
    "            false_color_img_data = response.read()\n",
    "        false_color_pil_image = Image.open(io.BytesIO(false_color_img_data))\n",
    "        false_color_buf = io.BytesIO()\n",
    "        false_color_pil_image.save(false_color_buf, format=\"JPEG\")\n",
    "        false_color_buf.seek(0)\n",
    "        false_color_image_base64 = base64.b64encode(false_color_buf.getvalue()).decode('utf-8')\n",
    "        false_color_buf.close()\n",
    "        print(\"False-color composite generated and encoded.\")\n",
    "        \n",
    "    except ee.EEException as e:\n",
    "        print(f\"GEE Error generating false-color composite: {e}\")\n",
    "        false_color_image_base64 = None\n",
    "    except urllib.error.URLError as e_url:\n",
    "        print(f\"URL Error downloading false-color composite: {e_url}\")\n",
    "        false_color_image_base64 = None\n",
    "    except Exception as e_fc:\n",
    "        print(f\"General error generating false-color composite: {e_fc}\")\n",
    "        false_color_image_base64 = None\n",
    "\n",
    "    # Displaying the images if show_image is True. Usage of local default image viewer:\n",
    "    if show_image:\n",
    "        if rgb_pil_image:\n",
    "            print(\"Displaying RGB thumbnail...\")\n",
    "            try:\n",
    "                rgb_pil_image.show() \n",
    "            except Exception as e_show:\n",
    "                print(f\"Could not display RGB thumbnail: {e_show}\")\n",
    "        \n",
    "        if ndvi_pil_image:\n",
    "            print(\"Displaying NDVI heatmap...\")\n",
    "            try:\n",
    "                ndvi_pil_image.show()\n",
    "            except Exception as e_show:\n",
    "                print(f\"Could not display NDVI heatmap: {e_show}\")\n",
    "        \n",
    "        if false_color_pil_image:\n",
    "            print(\"Displaying false-color composite...\")\n",
    "            try:\n",
    "                false_color_pil_image.show()\n",
    "            except Exception as e_show:\n",
    "                print(f\"Could not display false-color composite: {e_show}\")\n",
    "\n",
    "    # Compiling results with enhanced data structure... base64 images, metadata, stats, etc.\n",
    "    return {\n",
    "        \"image\": rgb_image_base64,        \n",
    "        \"ndvi_image\": ndvi_image_base64,\n",
    "        \"false_color_image\": false_color_image_base64, # For backward compatibility.\n",
    "        \n",
    "        # Enhanced metadata for AI processing:\n",
    "        \"image_metadata\": {\n",
    "            \"rgb_composite\": {\n",
    "                \"description\": \"Natural color composite using Red, Green, Blue bands\",\n",
    "                \"bands\": \"B4 (Red), B3 (Green), B2 (Blue)\",\n",
    "                \"purpose\": \"General landscape features and natural color interpretation\"\n",
    "            },\n",
    "            \"ndvi_heatmap\": {\n",
    "                \"description\": \"NDVI vegetation health heatmap with color mapping\",\n",
    "                \"color_mapping\": \"Brown/Red = bare soil/stressed vegetation (-0.2 to 0.3), Yellow/Green = healthy vegetation (0.3 to 0.8)\",\n",
    "                \"purpose\": \"Vegetation health assessment and stress identification\",\n",
    "                \"available\": ndvi_image_base64 is not None\n",
    "            },\n",
    "            \"false_color_composite\": {\n",
    "                \"description\": \"False-color composite highlighting vegetation patterns\",\n",
    "                \"bands\": \"B8 (NIR->Red), B4 (Red->Green), B3 (Green->Blue)\",\n",
    "                \"interpretation\": \"Red/Pink = vegetation, Blue/Black = water, Cyan/Blue = urban/bare soil\",\n",
    "                \"purpose\": \"Vegetation boundary detection and pattern analysis\",\n",
    "                \"available\": false_color_image_base64 is not None\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # Existing fields:\n",
    "        \"statistics\": all_stats,\n",
    "        \"roi_bounds\": roi_bounds,\n",
    "        \"gee_image_details\": (\n",
    "            f\"Median composite from COPERNICUS/S2_SR_HARMONIZED \"\n",
    "            f\"({gee_data.get('start_date')} to {gee_data.get('end_date')}), \"\n",
    "            f\"{gee_data.get('count')} images processed. \"\n",
    "            f\"Stats scale: {scale}m. Thumb: {thumb_dimensions}. \"\n",
    "            f\"Visualizations: RGB={'Present' if rgb_image_base64 else 'Absent'}, \"\n",
    "            f\"NDVI={'Present' if ndvi_image_base64 else 'Absent'}, \"\n",
    "            f\"False-color={'Present' if false_color_image_base64 else 'Absent'}.\"\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Workflow:\n",
    "\n",
    "The process for creating per-cell feature vectors from the GEDI, PRODES and SRTM data, which are then used for anomaly detection.\n",
    "\n",
    "Processes GEDI points by snapping them to a deterministic H3 grid, computing per-cell statistics, and then joining with PRODES and SRTM attributes. The output is normalized and ready for anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_GRID_CONFIG = {\n",
    "    'resolution': 9,\n",
    "    'min_shots_per_cell': 3,\n",
    "    'canopy_height_threshold': 2.0\n",
    "}\n",
    "# ===============================================================================\n",
    "# GRID SNAPPING GEDI POINTS TO H3 GRID\n",
    "# ===============================================================================\n",
    "def grid_snap(\n",
    "    gedi_df: pd.DataFrame,\n",
    "    resolution: int = DEFAULT_GRID_CONFIG['resolution'],\n",
    "    min_shots_per_cell: int = DEFAULT_GRID_CONFIG['min_shots_per_cell'],\n",
    "    canopy_height_threshold: float = DEFAULT_GRID_CONFIG['canopy_height_threshold']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Snap GEDI points to H3 grid and compute per-cell statistics.\n",
    "    Returns DataFrame with H3 cells and aggregated canopy statistics.\n",
    "    canopy_height_threshold: the minimum canopy height (meters) to consider a GEDI shot as valid.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Snapping {len(gedi_df)} GEDI shots to H3 grid (resolution {resolution})\")\n",
    "    if gedi_df.empty:\n",
    "        logger.warning(\"Empty GEDI DataFrame provided\")\n",
    "        return pd.DataFrame(columns=['h3_cell', 'lat', 'lon', 'shot_count', 'mean_canopy_height',\n",
    "                                   'std_canopy_height', 'min_canopy_height', 'max_canopy_height', 'mean_rh98',\n",
    "                                   'canopy_cover_ratio', 'height_variability'])\n",
    "    # Filtering out invalid shots, as well as shots with the canopy height below the given threshold:\n",
    "    valid_shots = gedi_df.dropna(subset=['lon', 'lat', 'rh98'])\n",
    "    valid_shots = valid_shots[valid_shots['canopy_height'] >= canopy_height_threshold]\n",
    "    if valid_shots.empty:\n",
    "        logger.warning(\"No valid GEDI shots after filtering\")\n",
    "        return pd.DataFrame(columns=['h3_cell', 'lat', 'lon', 'shot_count', 'mean_canopy_height',\n",
    "                                   'std_canopy_height', 'min_canopy_height', 'max_canopy_height', 'mean_rh98',\n",
    "                                   'canopy_cover_ratio', 'height_variability'])\n",
    "\n",
    "    # Converting each valid shot into H3 cells: resolution 9 ~174m edge length, ~0.11 km² (small neighborhood/block scale).\n",
    "    h3_cells = []\n",
    "    for _, row in valid_shots.iterrows():\n",
    "        try:\n",
    "            h3_cell = h3.latlng_to_cell(row['lat'], row['lon'], resolution)\n",
    "            h3_cells.append(h3_cell)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to convert coordinates to H3: {e}\")\n",
    "            h3_cells.append(None)\n",
    "    valid_shots = valid_shots.copy()\n",
    "    valid_shots['h3_cell'] = h3_cells\n",
    "    valid_shots = valid_shots.dropna(subset=['h3_cell'])\n",
    "    if valid_shots.empty:\n",
    "        logger.warning(\"No valid H3 cells generated\")\n",
    "        return pd.DataFrame(columns=['h3_cell', 'lat', 'lon', 'shot_count', 'mean_canopy_height',\n",
    "                                   'std_canopy_height', 'min_canopy_height', 'max_canopy_height', 'mean_rh98',\n",
    "                                   'canopy_cover_ratio', 'height_variability'])\n",
    "    \n",
    "    # Grouping valid_shots by h3_cell, then aggregating stats for canopy_height, rh98, lat, lon assigned to each cell:\n",
    "    cell_stats = valid_shots.groupby('h3_cell').agg({\n",
    "        'canopy_height': ['count', 'mean', 'std', 'min', 'max'],\n",
    "        'rh98': ['mean', 'std'],\n",
    "        'lat': 'mean',\n",
    "        'lon': 'mean'\n",
    "    }).round(4)\n",
    "    # Renaming columns for clarity:\n",
    "    cell_stats.columns = ['shot_count', 'mean_canopy_height', 'std_canopy_height', 'min_canopy_height', 'max_canopy_height',\n",
    "                         'mean_rh98', 'std_rh98', 'lat', 'lon']\n",
    "\n",
    "    # Filtering out cells with minimum shot count per cell:\n",
    "    cell_stats = cell_stats[cell_stats['shot_count'] >= min_shots_per_cell]\n",
    "    if cell_stats.empty:\n",
    "        logger.warning(f\"No cells meet minimum shot count requirement ({min_shots_per_cell})\")\n",
    "        return pd.DataFrame(columns=['h3_cell', 'lat', 'lon', 'shot_count', 'mean_canopy_height',\n",
    "                                   'std_canopy_height', 'min_canopy_height', 'max_canopy_height', 'mean_rh98',\n",
    "                                   'canopy_cover_ratio', 'height_variability'])\n",
    "    # Computing canopy cover ratio; normalizing the shot count per cell relative to the max shot count.\n",
    "    cell_stats['canopy_cover_ratio'] = (\n",
    "        cell_stats['shot_count'] / cell_stats['shot_count'].max()\n",
    "    ).round(4)\n",
    "    # Computing height variability, measuring how much the canopy height varies within each cell.\n",
    "    cell_stats['height_variability'] = (\n",
    "        cell_stats['std_canopy_height'] / (cell_stats['mean_canopy_height'] + 1e-6)\n",
    "    ).round(4)\n",
    "    # Filling in missing values for std_canopy_height and std_rh_98 with 0, then resetting the index for the DataFrame.\n",
    "    cell_stats['std_canopy_height'] = cell_stats['std_canopy_height'].fillna(0)\n",
    "    cell_stats['std_rh98'] = cell_stats['std_rh98'].fillna(0)\n",
    "    cell_stats = cell_stats.reset_index()\n",
    "\n",
    "    # Computing centroids for each H3 cell, which is essentially the lat and lon of the center of the H3 cell::\n",
    "    centroids = []\n",
    "    for h3_cell in cell_stats['h3_cell']:\n",
    "        try:\n",
    "            lat, lon = h3.cell_to_latlng(h3_cell)\n",
    "            centroids.append((lat, lon))\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to get centroid for H3 cell {h3_cell}: {e}\")\n",
    "            centroids.append((cell_stats[cell_stats['h3_cell'] == h3_cell]['lat'].iloc[0],\n",
    "                            cell_stats[cell_stats['h3_cell'] == h3_cell]['lon'].iloc[0]))\n",
    "            \n",
    "    # Converting the centroids into DataFrame, joining it with cell_stats:\n",
    "    centroid_df = pd.DataFrame(centroids, columns=['h3_lat', 'h3_lon'])\n",
    "    cell_stats = pd.concat([cell_stats.reset_index(drop=True), centroid_df], axis=1)\n",
    "\n",
    "    # Copying h3_lat, h3_lon derived from centroids into lat, lon, then dropping h3_lat, h3_lon:\n",
    "    cell_stats['lat'] = cell_stats['h3_lat']\n",
    "    cell_stats['lon'] = cell_stats['h3_lon']\n",
    "    cell_stats = cell_stats.drop(columns=['h3_lat', 'h3_lon'])\n",
    "    logger.info(f\"Generated {len(cell_stats)} H3 cells with sufficient GEDI coverage\")\n",
    "\n",
    "    # Finally, returning the cell_stats DataFrame:\n",
    "    return cell_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# SPATIAL JOIN H3 CELLS WITH SRTM ELEVATION AND SLOPE DATA\n",
    "# ===============================================================================\n",
    "def spatial_join_with_srtm(cell_stats: pd.DataFrame, srtm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform spatial join between H3 cells and SRTM elevation and slope data. Takes the cell_stats DataFrame\n",
    "    as well as the srtm_df DataFrame and returns an amalgamated DataFrame with the SRTM attributes joined to \n",
    "    the H3 cells. For each H3 cell, finds nearby SRTM points via a buffer, and computes terrain statistics.\n",
    "    \"\"\"\n",
    "    if cell_stats.empty or srtm_df.empty:\n",
    "        # Add SRTM columns with default values:\n",
    "        cell_stats['mean_elevation'] = 0.0\n",
    "        cell_stats['std_elevation'] = 0.0\n",
    "        cell_stats['mean_slope'] = 0.0\n",
    "        cell_stats['max_slope'] = 0.0\n",
    "        cell_stats['terrain_roughness'] = 0.0\n",
    "        cell_stats['elevation_norm'] = 0.0\n",
    "        return cell_stats\n",
    "    logger.info(f\"Joining {len(cell_stats)} H3 cells with {len(srtm_df)} SRTM points\")\n",
    "    \n",
    "    # Creating GeoDataFrames from H3 cells and SRTM data points respectively:\n",
    "    cell_points = [Point(lon, lat) for lon, lat in zip(cell_stats['lon'], cell_stats['lat'])]\n",
    "    cells_gdf = gpd.GeoDataFrame(cell_stats, geometry=cell_points, crs='EPSG:4326')\n",
    "    srtm_points = [Point(lon, lat) for lon, lat in zip(srtm_df['lon'], srtm_df['lat'])]\n",
    "    srtm_gdf = gpd.GeoDataFrame(srtm_df, geometry=srtm_points, crs='EPSG:4326')\n",
    "    \n",
    "    # For each H3 cell, find SRTM points within a reasonable distance.\n",
    "    # H3 resolution 9 has ~174m edge length, so use ~300m buffer to capture nearby points:\n",
    "    buffer_distance = 0.003  # ~300m in degrees (approximate).\n",
    "    \n",
    "    # Computing the terrain stats for each H3 cell, then joining the terrain stats to the cell_stats DataFrame:\n",
    "    terrain_stats = []\n",
    "    for idx, cell in cells_gdf.iterrows():\n",
    "        # Creating a buffer around the cell centroid, then finding SRTM points within that buffer:\n",
    "        cell_buffer = cell.geometry.buffer(buffer_distance)\n",
    "        nearby_srtm = srtm_gdf[srtm_gdf.geometry.within(cell_buffer)]\n",
    "        if len(nearby_srtm) > 0:\n",
    "            # Computing the elevation stats for each H3 cell, given the nearby SRTM points in the buffer:\n",
    "            elevation_stats = {\n",
    "                'mean_elevation': nearby_srtm['elevation'].mean(),\n",
    "                'std_elevation': nearby_srtm['elevation'].std() if len(nearby_srtm) > 1 else 0.0,\n",
    "                'mean_slope': nearby_srtm['slope_degrees'].mean(),\n",
    "                'max_slope': nearby_srtm['slope_degrees'].max(),\n",
    "                # Computing the terrain roughness; the standard deviation of the slope degrees:\n",
    "                'terrain_roughness': nearby_srtm['slope_degrees'].std() if len(nearby_srtm) > 1 else 0.0\n",
    "            }\n",
    "        else:\n",
    "            # No nearby SRTM points, use default values:\n",
    "            elevation_stats = {\n",
    "                'mean_elevation': 0.0,\n",
    "                'std_elevation': 0.0,\n",
    "                'mean_slope': 0.0,\n",
    "                'max_slope': 0.0,\n",
    "                'terrain_roughness': 0.0\n",
    "            }\n",
    "        terrain_stats.append(elevation_stats)\n",
    "    \n",
    "    # Converting the terrain stats to a DataFrame, then joining it with the cell_stats DataFrame:\n",
    "    terrain_df = pd.DataFrame(terrain_stats)\n",
    "    result = pd.concat([cell_stats.reset_index(drop=True), terrain_df], axis=1)\n",
    "    \n",
    "    # Computing normalized elevation for better canopy-elevation relationships:\n",
    "    # Normalize elevation within the local context to remove macro-relief effects:\n",
    "    if 'mean_elevation' in result.columns and result['mean_elevation'].std() > 0:\n",
    "        # Use z-score normalization to center around local mean:\n",
    "        elevation_mean = result['mean_elevation'].mean()\n",
    "        elevation_std = result['mean_elevation'].std()\n",
    "        result['elevation_norm'] = ((result['mean_elevation'] - elevation_mean) / elevation_std).round(4)\n",
    "    else:\n",
    "        # If no elevation variation, set normalized elevation to 0:\n",
    "        result['elevation_norm'] = 0.0\n",
    "    \n",
    "    # Rounding values for consistency:\n",
    "    terrain_columns = ['mean_elevation', 'std_elevation', 'mean_slope', 'max_slope', 'terrain_roughness']\n",
    "    for col in terrain_columns:\n",
    "        if col in result.columns:\n",
    "            result[col] = result[col].round(4)\n",
    "    logger.info(f\"Added terrain features to {len(result)} H3 cells.\")\n",
    "\n",
    "    # Finally, returning the resultant amalgamated DataFrame:\n",
    "    return result\n",
    "\n",
    "# ===============================================================================\n",
    "# SPATIAL JOIN H3 CELLS WITH PRODES DEFORESTATION POLYGONS\n",
    "# ===============================================================================\n",
    "def spatial_join_with_prodes(cell_stats: pd.DataFrame, prodes_gdf: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform spatial join between H3 cells and PRODES polygons. For each point in the cell_stats DataFrame,\n",
    "    check if it lies inside any of the polygons in the PRODES GeoDataFrame; if yes, copy the polygons' attrs\n",
    "    into the cell_stats DataFrame, then return the resultant amalgamated DataFrame.\n",
    "    \"\"\"\n",
    "    if cell_stats.empty or prodes_gdf.empty:\n",
    "        # Add PRODES columns with default values: \n",
    "        cell_stats['deforested'] = False\n",
    "        cell_stats['deforestation_year'] = None\n",
    "        cell_stats['deforestation_area_ha'] = 0.0\n",
    "        cell_stats['yrs_since_deforest'] = None\n",
    "        return cell_stats\n",
    "    logger.info(f\"Joining {len(cell_stats)} H3 cells with {len(prodes_gdf)} PRODES polygons\")\n",
    "    \n",
    "    # Create GeoDataFrame from H3 cells; 'cell_points': list of shapely Point objs, representing centroid of an H3 cell.\n",
    "    # Setting this to the 'geometry' column of the GeoDataFrame to enable spatial operations like spatial joins:\n",
    "    cell_points = [Point(lon, lat) for lon, lat in zip(cell_stats['lon'], cell_stats['lat'])]\n",
    "    cells_gdf = gpd.GeoDataFrame(cell_stats, geometry=cell_points, crs='EPSG:4326')\n",
    "        \n",
    "    # Ensure PRODES has consistent CRS:\n",
    "    if prodes_gdf.crs != 'EPSG:4326':\n",
    "        prodes_gdf = prodes_gdf.to_crs('EPSG:4326')\n",
    "        \n",
    "    # Spatial join: find which cells from 'cells_gdf' intersect with deforestation polygons in 'prodes_gdf'.\n",
    "    joined = gpd.sjoin(cells_gdf, prodes_gdf, how='left', predicate='intersects')\n",
    "        \n",
    "    # Handle multiple intersections per cell by retaining only the largest polygon in that cell.\n",
    "    if 'area_ha' in joined.columns and not joined['area_ha'].isna().all():\n",
    "        valid_areas = joined.dropna(subset=['area_ha'])\n",
    "        if not valid_areas.empty:\n",
    "            max_indices = valid_areas.groupby('h3_cell')['area_ha'].idxmax()\n",
    "            # Keeping only the row with max area_ha and those that did not have area_ha to begin with:\n",
    "            joined = pd.concat([\n",
    "                joined.loc[max_indices],\n",
    "                joined[~joined.index.isin(valid_areas.index)]\n",
    "            ]).drop_duplicates()\n",
    "        \n",
    "    # Computing years since deforestation for each cell, then creating a deforestation indicator:\n",
    "    current_year = datetime.now().year\n",
    "    joined['yrs_since_deforest'] = None\n",
    "    if 'year' in joined.columns:\n",
    "        mask = joined['year'].notna()\n",
    "        joined.loc[mask, 'yrs_since_deforest'] = current_year - joined.loc[mask, 'year']\n",
    "    joined['deforested'] = joined['year'].notna()\n",
    "        \n",
    "    # Renaming and selecting relevant columns:\n",
    "    column_mapping = {\n",
    "            'year': 'deforestation_year',\n",
    "            'area_ha': 'deforestation_area_ha'\n",
    "        }\n",
    "    for old_col, new_col in column_mapping.items():\n",
    "        if old_col in joined.columns:\n",
    "            joined[new_col] = joined[old_col]\n",
    "    joined['deforestation_area_ha'] = joined.get('deforestation_area_ha', 0.0).fillna(0.0)\n",
    "        \n",
    "    # Removing geometry and extra columns for return:\n",
    "    columns_to_keep = [\n",
    "            'h3_cell', 'lat', 'lon', 'shot_count', 'mean_canopy_height', 'std_canopy_height',\n",
    "            'min_canopy_height', 'max_canopy_height', 'mean_rh98', 'std_rh98', 'canopy_cover_ratio', 'height_variability',\n",
    "            'deforested', 'deforestation_year', 'deforestation_area_ha', 'yrs_since_deforest'\n",
    "        ]\n",
    "    result_columns = [col for col in columns_to_keep if col in joined.columns]\n",
    "    result = joined[result_columns].copy() \n",
    "\n",
    "    # Finally, returning the resultant amalgamated DataFrame:\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# ADDING DERIVED FEATURES TO PER-CELL DATAFRAME\n",
    "# ===============================================================================\n",
    "def add_derived_percell_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add derived features to the DataFrame and return the modified DataFrame.\"\"\"\n",
    "    # Computing the shot density; number of shots per unit area, where the unit area is the area of an H3 cell.\n",
    "    # For H3 resolution 9, the area of an H3 cell is ~0.105 km²:\n",
    "    h3_area_km2 = 0.105\n",
    "    df['shot_density'] = (df['shot_count'] / h3_area_km2).astype(float).round(4)\n",
    "    \n",
    "    # Computing the canopy height range; the true range (max - min) gives better spread information:\n",
    "    df['canopy_height_range'] = (df['max_canopy_height'] - df['min_canopy_height']).astype(float).round(4)  \n",
    "    \n",
    "    # Computing the rh98_height_difference; difference captures subtle variations between these similar metrics:\n",
    "    # RH98 and canopy height measure similar things, so difference reveals measurement inconsistencies or forest structure anomalies:\n",
    "    df['rh98_height_difference'] = (\n",
    "        df['mean_rh98'] - df['mean_canopy_height']    \n",
    "        ).astype(float).round(4)\n",
    "        \n",
    "    # Computing the deforestation impact proxy; decaying impact over time, where the impact is 1.0 for the first year,\n",
    "    # and decays by 1/2 every year thereafter:\n",
    "    if 'yrs_since_deforest' in df.columns:\n",
    "        # Ensure the column is numeric and handle NaN values properly\n",
    "        yrs_since = pd.to_numeric(df['yrs_since_deforest'], errors='coerce')\n",
    "        df['deforest_impact'] = np.where(\n",
    "            yrs_since.notna(),\n",
    "            1.0 / (yrs_since + 1),\n",
    "            0.0\n",
    "        )\n",
    "        df['deforest_impact'] = df['deforest_impact'].round(4)\n",
    "    else:\n",
    "        df['deforest_impact'] = 0.0\n",
    "    \n",
    "    # Computing terrain-based derived features if elevation data is available:\n",
    "    if 'mean_elevation' in df.columns and 'mean_slope' in df.columns:\n",
    "        # Computing the canopy-elevation relationship using normalized elevation to remove macro-relief effects:\n",
    "        # This gives a more meaningful relationship between local canopy and local terrain context:\n",
    "        if 'elevation_norm' in df.columns:\n",
    "            df['canopy_elevation_difference'] = (\n",
    "                df['mean_canopy_height'] - df['elevation_norm']\n",
    "            ).round(4)\n",
    "        else:\n",
    "            # Fallback to simple difference if normalization not available:\n",
    "            df['canopy_elevation_difference'] = (\n",
    "                df['mean_canopy_height'] - df['mean_elevation']\n",
    "            ).round(4)\n",
    "        \n",
    "        # Computing the slope-canopy interaction: how canopy varies with terrain slope:\n",
    "        df['slope_canopy_interaction'] = (\n",
    "            df['mean_slope'] * df['height_variability']\n",
    "        ).round(4)\n",
    "        \n",
    "        # Computing the terrain complexity index: combines slope and elevation variability:\n",
    "        df['terrain_complexity'] = (\n",
    "            (df['mean_slope'] * 0.6) + (df['terrain_roughness'] * 0.4)\n",
    "        ).round(4)\n",
    "    else:\n",
    "        # Default values if terrain data not available:\n",
    "        df['canopy_elevation_difference'] = 0.0\n",
    "        df['slope_canopy_interaction'] = 0.0\n",
    "        df['terrain_complexity'] = 0.0\n",
    "    \n",
    "    # Computing the anomaly potential score (pre-normalization); updated to include terrain features:\n",
    "    terrain_weight = 0.1 if 'mean_elevation' in df.columns else 0.0\n",
    "    base_weights_sum = 1.0 - terrain_weight # Ensuring the sum of weights is 1.0 even if terrain data is not available.\n",
    "    \n",
    "    df['anomaly_potential'] = (\n",
    "        df['height_variability'] * (0.3 * base_weights_sum) +\n",
    "        df['canopy_height_range'] * (0.2 * base_weights_sum) +\n",
    "        df['shot_density'] * (0.2 * base_weights_sum) +\n",
    "        df['deforest_impact'] * (0.3 * base_weights_sum) +\n",
    "        df['terrain_complexity'] * terrain_weight\n",
    "    ).round(4)\n",
    "    \n",
    "    # Finally, returning the DataFrame with the derived features:\n",
    "    return df\n",
    "    \n",
    "# ===============================================================================\n",
    "# NORMALIZING PER-CELL FEATURES FOR SCORING\n",
    "# ===============================================================================\n",
    "def normalize_percell_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize only per-cell features used in anomaly scoring pipeline.\n",
    "    Excludes regional features which should be handled separately.\n",
    "    \"\"\"\n",
    "    # Per-cell features that MUST be normalized for scoring:\n",
    "    percell_scoring_features = [\n",
    "        'height_variability', 'canopy_height_range', 'shot_density', \n",
    "        'rh98_height_difference', 'deforest_impact', 'terrain_complexity',\n",
    "        'canopy_elevation_difference', 'slope_canopy_interaction', \n",
    "        'mean_slope', 'anomaly_potential'\n",
    "    ]\n",
    "    \n",
    "    # Per-cell features to keep raw for LLM interpretability:\n",
    "    percell_context_features = [\n",
    "        'shot_count', 'mean_canopy_height', 'std_canopy_height', 'min_canopy_height', 'max_canopy_height',\n",
    "        'mean_rh98', 'std_rh98', 'canopy_cover_ratio',\n",
    "        'mean_elevation', 'std_elevation', 'elevation_norm', 'max_slope', 'terrain_roughness',\n",
    "        'deforested', 'deforestation_year', 'deforestation_area_ha', 'yrs_since_deforest'\n",
    "    ]\n",
    "    \n",
    "    # Filter scoring features to only include those present in the DataFrame:\n",
    "    available_scoring_features = [col for col in percell_scoring_features if col in df.columns]\n",
    "    if not available_scoring_features:\n",
    "        logger.warning(\"No per-cell scoring features available for normalization.\")\n",
    "        return df\n",
    "        \n",
    "    logger.info(f\"Normalizing {len(available_scoring_features)} per-cell scoring features, \"\n",
    "               f\"keeping {len([col for col in percell_context_features if col in df.columns])} context features raw\")\n",
    "\n",
    "    # Normalization process for per-cell scoring features only:\n",
    "    normalized_df = df.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    try:\n",
    "        # Handle potential issues with constant features or NaN values:\n",
    "        feature_data = df[available_scoring_features].fillna(0)\n",
    "        normalized_values = scaler.fit_transform(feature_data)\n",
    "        \n",
    "        # Ensure values are strictly in [0, 1] range:\n",
    "        normalized_values = np.clip(normalized_values, 0, 1)\n",
    "        \n",
    "        # Create normalized feature columns:\n",
    "        normalized_feature_df = pd.DataFrame(\n",
    "            normalized_values,\n",
    "            columns=[f\"{col}_norm\" for col in available_scoring_features],\n",
    "            index=df.index\n",
    "        )\n",
    "        normalized_df = pd.concat([normalized_df, normalized_feature_df], axis=1)\n",
    "        logger.info(f\"Successfully normalized {len(available_scoring_features)} per-cell features for scoring\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Per-cell feature normalization failed: {e}\")\n",
    "        # Add dummy normalized columns:\n",
    "        for col in available_scoring_features:\n",
    "            normalized_df[f\"{col}_norm\"] = df[col]\n",
    "    \n",
    "    # Finally, returning the normalized per-cell feature DataFrame:\n",
    "    return normalized_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PER-CELL FEATURE VECTOR BUILDING PIPELINE (spatially joined features only)\n",
    "# ===============================================================================\n",
    "def build_percell_vectors(cell_stats: pd.DataFrame, prodes_gdf: gpd.GeoDataFrame = None, \n",
    "                         srtm_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Per-cell feature vector building pipeline for only the spatially-joined features (SRTM, PRODES).\n",
    "    These features vary meaningfully between H3 cells based on local spatial characteristics.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Building per-cell feature vectors for {len(cell_stats)} H3 cells\")\n",
    "    \n",
    "    if cell_stats.empty:\n",
    "        logger.warning(\"Empty cell statistics provided\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with GEDI-derived cell stats (already per-cell):\n",
    "    feature_df = cell_stats.copy()\n",
    "    \n",
    "    # Join with SRTM data if available (per-cell spatial join):\n",
    "    if srtm_df is not None and not srtm_df.empty:\n",
    "        logger.info(f\"Spatially joining with {len(srtm_df)} SRTM elevation points\")\n",
    "        feature_df = spatial_join_with_srtm(feature_df, srtm_df)\n",
    "    else:\n",
    "        logger.warning(\"No SRTM data provided, adding default terrain features\")\n",
    "        terrain_columns = ['mean_elevation', 'std_elevation', 'mean_slope', 'max_slope', 'terrain_roughness', 'elevation_norm']\n",
    "        for col in terrain_columns:\n",
    "            feature_df[col] = 0.0\n",
    "        \n",
    "    # Join with PRODES data if available (per-cell spatial join):\n",
    "    if prodes_gdf is not None and not prodes_gdf.empty:\n",
    "        logger.info(f\"Spatially joining with {len(prodes_gdf)} PRODES polygons\")\n",
    "        feature_df = spatial_join_with_prodes(feature_df, prodes_gdf)\n",
    "    else:\n",
    "        logger.warning(\"No PRODES data provided, adding default deforestation features\")\n",
    "        feature_df['deforested'] = False\n",
    "        feature_df['deforestation_year'] = None\n",
    "        feature_df['deforestation_area_ha'] = 0.0\n",
    "        feature_df['yrs_since_deforest'] = None\n",
    "        \n",
    "    # Add derived features based on per-cell data, and then do per-cell normalization of pertinent features:\n",
    "    derived_df = add_derived_percell_features(feature_df)\n",
    "    normalized_df = normalize_percell_features(derived_df)\n",
    "    \n",
    "    # Finally, returning the normalized per-cell feature DataFrame:\n",
    "    logger.info(f\"Generated per-cell feature vectors with {len(normalized_df.columns)} features\")\n",
    "    return normalized_df\n",
    "\n",
    "# ================================================================================================\n",
    "# COMPLETE FEATURE VECTOR BUILDING PIPELINE (gridsnap, per-cell feature vector building)\n",
    "# ================================================================================================\n",
    "def feat_engineering_pipeline(gedi_df: pd.DataFrame, prodes_gdf: gpd.GeoDataFrame = None, srtm_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The complete feature engineering pipeline for the spatially joined, per-cell feature vectors.\n",
    "    Takes the GEDI DataFrame, PRODES GeoDataFrame, and SRTM DataFrame as inputs, snaps the GEDI \n",
    "    shots to an H3 grid, perform the per-cell feature engineering, then finally returning a \n",
    "    DataFrame with the spatially joined, per-cell feature vectors.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting per-cell feature engineering pipeline\")\n",
    "    \n",
    "    # Step 1: Grid snapping; snapping the GEDI shots to an H3 grid:\n",
    "    cell_stats = grid_snap(gedi_df)\n",
    "    if cell_stats.empty:\n",
    "        logger.warning(\"No H3 cells generated from GEDI data.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Step 2: Build per-cell feature vectors (spatially-joined features only):\n",
    "    percell_features = build_percell_vectors(cell_stats, prodes_gdf, srtm_df)\n",
    "    \n",
    "    logger.info(f\"Feature engineering complete: {len(percell_features)} cells × {len(percell_features.columns)} features\")\n",
    "    \n",
    "    # Finally, returning the per-cell feature vector DataFrame:\n",
    "    return percell_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Pipeline\n",
    "\n",
    "The anomaly detection process that takes the per-cell feature vectors and scores them for anomalies.\n",
    "The two methods that can be used are simple linear combination of features (weighted) and isolation forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERCELL_FEATURES: Dict[str, float] = {\n",
    "    \"height_variability_norm\": 0.15,\n",
    "    \"canopy_height_range_norm\": 0.15,\n",
    "    \"shot_density_norm\": 0.12,\n",
    "    \"rh98_height_difference_norm\": 0.08,\n",
    "    \"deforest_impact_norm\": 0.18,\n",
    "    \"terrain_complexity_norm\": 0.12,\n",
    "    \"canopy_elevation_difference_norm\": 0.10,\n",
    "    \"slope_canopy_interaction_norm\": 0.08,\n",
    "    \"mean_slope_norm\": 0.02,\n",
    "}\n",
    "# ===============================================================================\n",
    "# SCORING FUNCTIONS: LINEAR COMBINATION AND ISOLATION FOREST\n",
    "# ===============================================================================\n",
    "def weighted_score(df: pd.DataFrame, weights: Dict[str, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computing simple weighted linear combination scores for the features.\n",
    "    Takes the feature DataFrame and the weights dictionary containing the weights \n",
    "    for each respective feature, then returns and array of the weighted scores.\n",
    "    \"\"\"\n",
    "    score = np.zeros(len(df))\n",
    "    for feat, weight in weights.items():\n",
    "        if feat in df.columns:\n",
    "            feature_values = df[feat].fillna(0).to_numpy()\n",
    "            score += weight * feature_values\n",
    "        else:\n",
    "            logger.warning(f\"Feature '{feat}' missing; weight ignored.\")    \n",
    "    return score\n",
    "\n",
    "def iforest_score(df: pd.DataFrame, features: List[str], \n",
    "                  contamination: float = 0.05, random_state: int = 424242) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computation of the anomaly scores via the Isolation Forest model. Takes the \n",
    "    feature DataFrame and the list of features to use, then returns normalized \n",
    "    anomaly scores where higher values indicate more anomalous cells.\n",
    "    \"\"\"\n",
    "    # Preparing the feature matrix, then fitting the Isolation Forest model:\n",
    "    X = df[features].fillna(0)\n",
    "    iforest = IsolationForest(\n",
    "        n_estimators=100, \n",
    "        contamination=contamination, \n",
    "        random_state=random_state,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    iforest.fit(X)\n",
    "    \n",
    "    # Getting the anomaly scores (inverting so higher = more anomalous), then normalizing to [0, 1] range:\n",
    "    raw_scores = -iforest.score_samples(X)\n",
    "    norm_scores = (raw_scores - raw_scores.min()) / (raw_scores.max() - raw_scores.min() + 1e-9)\n",
    "    \n",
    "    # Finally, returning the normalized anomaly scores:\n",
    "    return norm_scores\n",
    "\n",
    "# ===============================================================================\n",
    "# MAIN SCORING AND RANKING PIPELINE\n",
    "# ===============================================================================\n",
    "\n",
    "def score_cells(percell_df: pd.DataFrame, method: str = 'weighted', **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Adding the computed anomaly scores to the per-cell DataFrame.\"\"\"\n",
    "    result_df = percell_df.copy()\n",
    "    \n",
    "    if method == 'weighted':\n",
    "        weights = kwargs.get('weights', PERCELL_FEATURES)\n",
    "        result_df['score'] = weighted_score(percell_df, weights)\n",
    "        \n",
    "    elif method == 'iforest':\n",
    "        feature_cols = kwargs.get('feature_cols')\n",
    "        if feature_cols is None:\n",
    "            logger.warning(\"No feature columns specified; using all available features in PERCELL_FEATURES dict.\")\n",
    "            feature_cols = [col for col in PERCELL_FEATURES.keys() if col in percell_df.columns]\n",
    "        \n",
    "        result_df['score'] = iforest_score(\n",
    "            percell_df, \n",
    "            feature_cols,\n",
    "            kwargs.get('contamination', 0.05),\n",
    "            kwargs.get('random_state', 424242)\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scoring method: {method}\")\n",
    "    \n",
    "    # Finally, returning the DataFrame with the added 'score' column:\n",
    "    return result_df\n",
    "\n",
    "def rank_cells(df: pd.DataFrame, n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the per-cell DataFrame (which contains the anomaly scores), as well as the \n",
    "    number of top N cells to return, and then returns the top N cells, sorted in descending\n",
    "    order.\n",
    "    \"\"\"\n",
    "    if 'score' not in df.columns:\n",
    "        raise ValueError(\"Score column not found. Call score_cells() first.\")\n",
    "    return df.sort_values('score', ascending=False).head(n).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Integration:\n",
    "\n",
    "Process for comprehensive anomalous archaeological feature detection.\n",
    "\n",
    "Centralized prompting utilities that sends the top-N anomaly H3 cells, along with regional LiDAR and \n",
    "Sentinel-2 context to either OpenAI or OpenRouter models via its APIs. The model returns a structured and \n",
    "comprehensive assessment indicating whether each cell is likely to contain any anomalous archaeological\n",
    "features, all in a JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "LOG_PATH = Path(os.getenv(\"LLM_LOG_PATH\", \"llm_prompt_log.jsonl\"))\n",
    "\n",
    "# Message builder for the provided LLM:\n",
    "def build_messages(cell: Dict[str, Any], lidar_s2_ctx: Dict[str, Any], regional_assessment: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Function that facilitates the construction of a multimodal message list, suitable for both OpenAI and\n",
    "    OpenRouter models. \n",
    "    \"\"\"\n",
    "    # Cell text; per-cell feature vector from GEDI, STRM, PRODES + per-cell instructions, in JSON format:\n",
    "    cell_text  =  \"### Anomaly Cell Feature Vector\\n\" \\\n",
    "        + json.dumps(cell, indent=2) \\\n",
    "        + f\"\\n\\n{CELL_INSTRUCTIONS}\"\n",
    "    user_content: List[Dict[str, Any]] = []\n",
    "\n",
    "    # If a regional assessment is provided, prepend it to the user content\n",
    "    if regional_assessment is not None:\n",
    "        user_content.append({\"type\": \"text\", \"text\": f\"### Regional Assessment (LLM-generated):\\n{regional_assessment}\\n\"})\n",
    "\n",
    "    user_content.append({\"type\": \"text\", \"text\": cell_text})\n",
    "\n",
    "    # Images from regional LiDAR and Sentinel-2 composites:\n",
    "    for key in (\"image\", \"ndvi_image\", \"false_color_image\"):\n",
    "        b64_img = lidar_s2_ctx.get(key)\n",
    "        if b64_img:\n",
    "            user_content.append({\"type\": \"image_url\",\n",
    "                                 \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_img}\"}})\n",
    "\n",
    "    # Statistics and metadata from regionalLiDAR and Sentinel-2 composites, in JSON format:\n",
    "    stats_text = \"\\n\\n### Regional LiDAR and Sentinel-2 Statistics\\n\" \\\n",
    "        + json.dumps(lidar_s2_ctx.get(\"statistics\", {}), indent=2)\n",
    "    user_content.append({\"type\": \"text\", \"text\": stats_text})\n",
    "\n",
    "    # Finally, returning the complete message for the LLM; system + user content:\n",
    "    return [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, \n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "           ]\n",
    "\n",
    "# ===============================================================================\n",
    "# OPENAI API MODEL CALL\n",
    "# ===============================================================================\n",
    "def openai_model_call(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 1.0,\n",
    "    max_tokens: int = 128000,   \n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sends the provided message(s) to a specified OpenAI model via its API, and returns the LLM's response.\n",
    "    \"\"\"\n",
    "    if openai_client is None:\n",
    "        raise ValueError(\"OpenAI client is not initialized and/or API key is missing. Cannot call OpenAI API.\")\n",
    "    try:\n",
    "        logger.info(f\"Sending request to OpenAI Responses API for model: {model_name}...\")\n",
    "        llm_response = openai_client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            **kwargs\n",
    "        )\n",
    "        logger.info(\"Response received from OpenAI Chat Completions API.\")\n",
    "        return llm_response.choices[0].message.content.strip()\n",
    "    except OpenAIError as e:\n",
    "        return f\"[OpenAI API error] {str(e)}\"   \n",
    "\n",
    "# ===============================================================================\n",
    "# OPENROUTER API MODEL CALL\n",
    "# ===============================================================================\n",
    "def openrouter_model_call(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 1.0,\n",
    "    max_tokens: int = 128000,   \n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sends the provided message(s) to a specified OpenRouter model via its API, and returns the LLM's response.\n",
    "    \"\"\"\n",
    "    if not OPENROUTER_API_KEY:\n",
    "        return \"[OpenRouter API error] OpenRouter API key is missing.\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }    \n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Sending request to OpenRouter API for model: {model_name}...\")\n",
    "        llm_response = requests.post(\"https://openrouter.ai/api/v1/chat/completions\",headers=headers, json=payload, timeout=120)\n",
    "        logger.info(f\"Response received from OpenRouter API: {llm_response.status_code}\")\n",
    "        llm_response_json = llm_response.json()\n",
    "        if llm_response.status_code != 200:\n",
    "            return f\"[OpenRouter API error] {llm_response_json.get('error', llm_response.text)}\"\n",
    "        return llm_response_json[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[OpenRouter API error] {str(e)}\"\n",
    "\n",
    "# ===============================================================================\n",
    "# UNIFIED WRAPPER FOR API MODEL CALLS\n",
    "# ===============================================================================\n",
    "def llm_model_call(\n",
    "    messages: List[Dict[str, Any]],\n",
    "    provider: str = OPENAI_PROVIDER,    \n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 1.0,\n",
    "    max_tokens: int = 128000,   \n",
    "    **kwargs,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Convenient unified wrapper for the API model calls. Sends the already-build chat messages to the \n",
    "    chosen LLM provider, and returns the LLM's response.\n",
    "    \"\"\"\n",
    "    if provider == OPENAI_PROVIDER:\n",
    "        model_name = model_name or OPENAI_DEFAULT_MODEL\n",
    "        return openai_model_call(messages, model_name, temperature, max_tokens, **kwargs)\n",
    "    elif provider == OPENROUTER_PROVIDER:\n",
    "        model_name = model_name or OPENROUTER_DEFAULT_MODEL\n",
    "        return openrouter_model_call(messages, model_name, temperature, max_tokens, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown provider: {provider}\")\n",
    "    \n",
    "# ===============================================================================\n",
    "# EFFICIENT BATCH ANALYSIS: SEND REGIONAL CONTEXT ONCE FOR ALL CELLS\n",
    "# ===============================================================================\n",
    "def build_regional_context_message(lidar_s2_ctx: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Build a message for the LLM to assess only the regional LiDAR and Sentinel-2 context.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a senior Amazonian archaeologist and remote-sensing scientist. \"\n",
    "        \"Given ONLY the following regional LiDAR and Sentinel-2 statistics, images, and metadata, \"\n",
    "        \"summarize the regional context and highlight any features, anomalies, or patterns that could \"\n",
    "        \"influence archaeological potential in the area. Return a concise summary that can be used as context for further cell-level analysis.\"\n",
    "    )\n",
    "    user_content: List[Dict[str, Any]] = []\n",
    "    # Images from regional LiDAR and Sentinel-2 composites:\n",
    "    for key in (\"image\", \"ndvi_image\", \"false_color_image\"):\n",
    "        b64_img = lidar_s2_ctx.get(key)\n",
    "        if b64_img:\n",
    "            user_content.append({\"type\": \"image_url\",\n",
    "                                 \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_img}\"}})\n",
    "    # Statistics and metadata from regionalLiDAR and Sentinel-2 composites, in JSON format:\n",
    "    stats_text = \"\\n\\n### Regional LiDAR and Sentinel-2 Statistics\\n\" \\\n",
    "        + json.dumps(lidar_s2_ctx.get(\"statistics\", {}), indent=2)\n",
    "    user_content.append({\"type\": \"text\", \"text\": stats_text})\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "def analyze_top_n_cells_batch(\n",
    "    top_n_cells: List[Dict[str, Any]],\n",
    "    lidar_s2_ctx: Dict[str, Any],\n",
    "    provider: str = OPENROUTER_PROVIDER,\n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 1.0,\n",
    "    max_tokens: int = 128000,\n",
    "    save_log: bool = True,\n",
    "    **kwargs,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    New batch analysis: first, LLM assesses regional LiDAR/Sentinel-2 context; then, each cell is assessed using this regional summary as context.\n",
    "    Returns a dict with the regional assessment and all cell assessments, plus logging metadata.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Step 1: Sending regional context to {provider} API...\")\n",
    "    regional_messages = build_regional_context_message(lidar_s2_ctx)\n",
    "    regional_assessment = llm_model_call(regional_messages, provider, model_name, temperature, max_tokens, **kwargs)\n",
    "\n",
    "    logger.info(\"Step 2: Assessing each cell with regional context...\")\n",
    "    cell_results = []\n",
    "    for idx, cell in enumerate(top_n_cells):\n",
    "        cell_id = cell.get(\"h3_cell\", f\"cell_{idx+1}\")\n",
    "        messages = build_messages(cell, lidar_s2_ctx, regional_assessment=regional_assessment)\n",
    "        llm_response = llm_model_call(messages, provider, model_name, temperature, max_tokens, **kwargs)\n",
    "        entry = {\n",
    "            \"cell_id\": cell_id,\n",
    "            \"llm_response\": llm_response,\n",
    "            \"messages\": messages\n",
    "        }\n",
    "        cell_results.append(entry)\n",
    "\n",
    "    batch_entry = {\n",
    "        \"batch_id\": f\"batch_{len(top_n_cells)}_cells\",\n",
    "        \"cell_ids\": [cell.get(\"h3_cell\", f\"cell_{idx+1}\") for idx, cell in enumerate(top_n_cells)],\n",
    "        \"provider\": provider,\n",
    "        \"model_name\": model_name or (OPENAI_DEFAULT_MODEL if provider == OPENAI_PROVIDER else OPENROUTER_DEFAULT_MODEL),\n",
    "        \"regional_assessment\": regional_assessment,\n",
    "        \"cell_assessments\": cell_results,\n",
    "        \"num_cells\": len(top_n_cells)\n",
    "    }\n",
    "\n",
    "    # Save batch entry to log\n",
    "    if save_log:\n",
    "        with LOG_PATH.open(\"a\") as entry_path:\n",
    "            entry_path.write(json.dumps(batch_entry) + \"\\n\")\n",
    "        logger.info(f\"Logged batch analysis of {len(top_n_cells)} cells (regional + per-cell) to {LOG_PATH}\")\n",
    "\n",
    "    return batch_entry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Pipeline Execution:\n",
    "\n",
    "Execution of the full anomaly detection pipeline, all done step by step.\n",
    "1. Fetching LiDAR and Sentinel-2 data.\n",
    "2. Fetching GEDI, PRODES and SRTM data.\n",
    "3. Extracting features from the LiDAR and Sentinel-2 data.\n",
    "4. Creating per-cell feature vectors from the GEDI, PRODES and SRTM data.\n",
    "5. Scoring and ranking the per-cell feature vectors for anomalies.\n",
    "7. Running LLM-based Assessment.\n",
    "8. Displaying the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Fetch GEDI, PRODES, and SRTM Data ---\n",
    "print(\"--- Step 1: Fetching GEDI, PRODES, SRTM data ---\")\n",
    "gedi_df, prodes_gdf, srtm_df = gedi_prodes_srtm_fetch_pipeline(\n",
    "    BBOX, YEAR, CACHE_DIR, FORCE_REFRESH\n",
    ")\n",
    "print(f\"Fetched {len(gedi_df)} GEDI shots, {len(prodes_gdf)} PRODES polygons, {len(srtm_df)} SRTM points\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- 2. Fetch Regional LiDAR and Sentinel-2 Data ---\n",
    "print(\"--- Step 2: Fetching regional LiDAR and Sentinel-2 data ---\")\n",
    "region_ctx = lidar_sentinel2_fetch_pipeline(BBOX)\n",
    "print(f\"LiDAR path: {region_ctx.get('lidar_path')}\")\n",
    "print(f\"S2 data available: {region_ctx.get('s2_data') is not None}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 3. Extract Regional Features and Display Images ---\n",
    "print(\"--- Step 3: Extracting regional features and displaying images ---\")\n",
    "region_features = {}\n",
    "if region_ctx and region_ctx.get(\"lidar_path\"):\n",
    "    lidar_stats = lidar_ot_extract_features(region_ctx[\"lidar_path\"], show_image=SHOW_IMAGES)\n",
    "    if lidar_stats and lidar_stats.get(\"image\"):\n",
    "        print(\"Displaying LiDAR Elevation and Hillshade:\")\n",
    "        display(IPImage(base64.b64decode(lidar_stats[\"image\"])))\n",
    "        region_features.update(lidar_stats)\n",
    "\n",
    "if region_ctx and region_ctx.get(\"s2_data\"):\n",
    "    s2_stats = sentinel2_gee_extract_features(region_ctx[\"s2_data\"], show_image=SHOW_IMAGES)\n",
    "    if s2_stats:\n",
    "        if s2_stats.get(\"image\"):\n",
    "            print(\"Displaying Sentinel-2 RGB Composite:\")\n",
    "            display(IPImage(base64.b64decode(s2_stats[\"image\"])))\n",
    "        if s2_stats.get(\"ndvi_image\"):\n",
    "            print(\"Displaying Sentinel-2 NDVI Heatmap:\")\n",
    "            display(IPImage(base64.b64decode(s2_stats[\"ndvi_image\"])))\n",
    "        if s2_stats.get(\"false_color_image\"):\n",
    "            print(\"Displaying Sentinel-2 False-Color Composite:\")\n",
    "            display(IPImage(base64.b64decode(s2_stats[\"false_color_image\"])))\n",
    "        region_features.update(s2_stats)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 4. Perform Feature Engineering ---\n",
    "print(\"--- Step 4: Engineering per-cell features ---\")\n",
    "features_df = feat_engineering_pipeline(gedi_df, prodes_gdf, srtm_df)\n",
    "print(f\"Generated {len(features_df)} per-cell feature vectors.\")\n",
    "print(\"Sample of engineered features:\")\n",
    "display(features_df.head())\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Score and Rank Anomalies ---\n",
    "print(\"--- Step 5: Scoring and ranking anomalies ---\")\n",
    "scored_df = score_cells(features_df, method='weighted')\n",
    "top_cells_df = rank_cells(scored_df, n=TOP_N)\n",
    "print(f\"Top {TOP_N} ranked anomalies:\")\n",
    "display(top_cells_df[['h3_cell', 'score', 'lat', 'lon', 'mean_canopy_height', 'height_variability', 'deforest_impact']])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Run LLM-Based Assessment ---\n",
    "print(f\"--- Step 6: Running LLM-based assessment for top {TOP_N} cells ---\")\n",
    "llm_results = analyze_top_n_cells_batch(\n",
    "    top_cells_df.to_dict(orient='records'),\n",
    "    region_features,\n",
    "    provider=LLM_PROVIDER,\n",
    "    model_name=LLM_MODEL,\n",
    "    save_log=True\n",
    ")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# --- 7. Display Final Results ---\n",
    "print(\"--- Step 7: Displaying Final Assessment Results ---\")\n",
    "\n",
    "if llm_results:\n",
    "    # Display Regional Assessment\n",
    "    display(Markdown(\"## Regional Assessment (from LLM)\"))\n",
    "    display(Markdown(llm_results.get('regional_assessment', 'No regional assessment provided.')))\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Display Per-Cell Assessments\n",
    "    cell_assessments = llm_results.get(\"cell_assessments\", [])\n",
    "    for i, entry in enumerate(cell_assessments):\n",
    "        cell_id = entry.get('cell_id', f'Cell {i+1}')\n",
    "        display(Markdown(f\"### LLM Assessment for {cell_id} (Rank {i+1})\"))\n",
    "        \n",
    "        try:\n",
    "            # The response is often a stringified JSON, so we parse it\n",
    "            response_json = json.loads(entry.get('llm_response', '{}'))\n",
    "            display(JSON(response_json))\n",
    "        except json.JSONDecodeError:\n",
    "            # If it's not JSON, display as markdown\n",
    "            display(Markdown(entry.get('llm_response', 'No response')))\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "else:\n",
    "    print(\"LLM analysis was not run or failed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
